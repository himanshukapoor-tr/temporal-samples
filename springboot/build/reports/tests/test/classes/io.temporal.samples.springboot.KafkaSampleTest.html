<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>
<meta http-equiv="x-ua-compatible" content="IE=edge"/>
<title>Test results - KafkaSampleTest</title>
<link href="../css/base-style.css" rel="stylesheet" type="text/css"/>
<link href="../css/style.css" rel="stylesheet" type="text/css"/>
<script src="../js/report.js" type="text/javascript"></script>
</head>
<body>
<div id="content">
<h1>KafkaSampleTest</h1>
<div class="breadcrumbs">
<a href="../index.html">all</a> &gt; 
<a href="../packages/io.temporal.samples.springboot.html">io.temporal.samples.springboot</a> &gt; KafkaSampleTest</div>
<div id="summary">
<table>
<tr>
<td>
<div class="summaryGroup">
<table>
<tr>
<td>
<div class="infoBox" id="tests">
<div class="counter">1</div>
<p>tests</p>
</div>
</td>
<td>
<div class="infoBox" id="failures">
<div class="counter">0</div>
<p>failures</p>
</div>
</td>
<td>
<div class="infoBox" id="ignored">
<div class="counter">0</div>
<p>ignored</p>
</div>
</td>
<td>
<div class="infoBox" id="duration">
<div class="counter">0.152s</div>
<p>duration</p>
</div>
</td>
</tr>
</table>
</div>
</td>
<td>
<div class="infoBox success" id="successRate">
<div class="percent">100%</div>
<p>successful</p>
</div>
</td>
</tr>
</table>
</div>
<div id="tabs">
<ul class="tabLinks">
<li>
<a href="#tab0">Tests</a>
</li>
<li>
<a href="#tab1">Standard output</a>
</li>
</ul>
<div id="tab0" class="tab">
<h2>Tests</h2>
<table>
<thead>
<tr>
<th>Test</th>
<th>Duration</th>
<th>Result</th>
</tr>
</thead>
<tr>
<td class="success">testKafkaWorflow()</td>
<td class="success">0.152s</td>
<td class="success">passed</td>
</tr>
</table>
</div>
<div id="tab1" class="tab">
<h2>Standard output</h2>
<span class="code">
<pre>2023-11-17 11:05:54.784  INFO 80454 --- [    Test worker] .b.t.c.SpringBootTestContextBootstrapper : Neither @ContextConfiguration nor @ContextHierarchy found for test class [io.temporal.samples.springboot.KafkaSampleTest], using SpringBootContextLoader
2023-11-17 11:05:54.784  INFO 80454 --- [    Test worker] o.s.t.c.support.AbstractContextLoader    : Could not detect default resource locations for test class [io.temporal.samples.springboot.KafkaSampleTest]: no resource found for suffixes {-context.xml, Context.groovy}.
2023-11-17 11:05:54.785  INFO 80454 --- [    Test worker] .b.t.c.SpringBootTestContextBootstrapper : Loaded default TestExecutionListener class names from location [META-INF/spring.factories]: [org.springframework.boot.test.autoconfigure.restdocs.RestDocsTestExecutionListener, org.springframework.boot.test.autoconfigure.web.client.MockRestServiceServerResetTestExecutionListener, org.springframework.boot.test.autoconfigure.web.servlet.MockMvcPrintOnlyOnFailureTestExecutionListener, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverTestExecutionListener, org.springframework.boot.test.autoconfigure.webservices.client.MockWebServiceServerTestExecutionListener, org.springframework.boot.test.mock.mockito.MockitoTestExecutionListener, org.springframework.boot.test.mock.mockito.ResetMocksTestExecutionListener, org.springframework.test.context.web.ServletTestExecutionListener, org.springframework.test.context.support.DirtiesContextBeforeModesTestExecutionListener, org.springframework.test.context.event.ApplicationEventsTestExecutionListener, org.springframework.test.context.support.DependencyInjectionTestExecutionListener, org.springframework.test.context.support.DirtiesContextTestExecutionListener, org.springframework.test.context.transaction.TransactionalTestExecutionListener, org.springframework.test.context.jdbc.SqlScriptsTestExecutionListener, org.springframework.test.context.event.EventPublishingTestExecutionListener]
2023-11-17 11:05:54.786  INFO 80454 --- [    Test worker] .b.t.c.SpringBootTestContextBootstrapper : Using TestExecutionListeners: [org.springframework.test.context.web.ServletTestExecutionListener@54cb2cee, org.springframework.test.context.support.DirtiesContextBeforeModesTestExecutionListener@12ef8410, org.springframework.test.context.event.ApplicationEventsTestExecutionListener@731819a0, org.springframework.boot.test.mock.mockito.MockitoTestExecutionListener@71bebddf, org.springframework.boot.test.autoconfigure.SpringBootDependencyInjectionTestExecutionListener@7c1f435a, org.springframework.test.context.support.DirtiesContextTestExecutionListener@123e87cd, org.springframework.test.context.transaction.TransactionalTestExecutionListener@21d85933, org.springframework.test.context.jdbc.SqlScriptsTestExecutionListener@250c5892, org.springframework.test.context.event.EventPublishingTestExecutionListener@238fcafa, org.springframework.boot.test.autoconfigure.restdocs.RestDocsTestExecutionListener@45e931a9, org.springframework.boot.test.autoconfigure.web.client.MockRestServiceServerResetTestExecutionListener@269bf9e6, org.springframework.boot.test.autoconfigure.web.servlet.MockMvcPrintOnlyOnFailureTestExecutionListener@6114ed6f, org.springframework.boot.test.autoconfigure.web.servlet.WebDriverTestExecutionListener@3c1133ad, org.springframework.boot.test.autoconfigure.webservices.client.MockWebServiceServerTestExecutionListener@30f929ff, org.springframework.boot.test.mock.mockito.ResetMocksTestExecutionListener@49d4a2cf]

  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::               (v2.7.12)

2023-11-17 11:05:54.861  INFO 80454 --- [    Test worker] k.utils.Log4jControllerRegistration$     : Registered kafka:type=kafka.Log4jController MBean
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : 
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :   ______                  _                                          
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :  |___  /                 | |                                         
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :     / /    ___     ___   | | __   ___    ___   _ __     ___   _ __   
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :    / /    / _ \   / _ \  | |/ /  / _ \  / _ \ | '_ \   / _ \ | '__|
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :   / /__  | (_) | | (_) | |   &lt;  |  __/ |  __/ | |_) | |  __/ | |    
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :  /_____|  \___/   \___/  |_|\_\  \___|  \___| | .__/   \___| |_|
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :                                               | |                     
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     :                                               |_|                     
2023-11-17 11:05:54.867  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : 
2023-11-17 11:05:54.875  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2023-11-17 11:05:54.875  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:host.name=himanshukapoor
2023-11-17 11:05:54.875  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.version=17.0.4
2023-11-17 11:05:54.875  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.vendor=Homebrew
2023-11-17 11:05:54.875  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.home=/opt/homebrew/Cellar/openjdk@17/17.0.4/libexec/openjdk.jdk/Contents/Home
2023-11-17 11:05:54.875  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.class.path=/Users/himanshukapoor/.gradle/caches/8.0.1/workerMain/gradle-worker.jar:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/classes/java/test:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/resources/test:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/classes/java/main:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/resources/main:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-spring-boot-starter-alpha/1.22.0/a1420aa75dfe1c3ea661aff7eba4182e63f63ada/temporal-spring-boot-starter-alpha-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-spring-boot-autoconfigure-alpha/1.22.0/13f215641f46bd927a4d1ab7acd44a25e632eae1/temporal-spring-boot-autoconfigure-alpha-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/2.7.12/aba3f96331e2cfa316d8ec7446804721120a0552/spring-boot-starter-web-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-thymeleaf/2.7.12/2abc3b0955eb7aee9ae4a97b3c8e660b9f7846fb/spring-boot-starter-thymeleaf-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-actuator/2.7.12/f3f66262d4b856db3afbb55e4c04446b24b085d6/spring-boot-starter-actuator-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-data-jpa/2.7.12/17fdcc9bc155a6e6459190078c3a37c7fce0ced8/spring-boot-starter-data-jpa-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/2.8.11/7068d24ea230111777ce60232fa39d4e570d4709/spring-kafka-2.8.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/2.8.11/593e579e7671ee39c52c61ae23df451d6ad1cefd/spring-kafka-test-2.8.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-testing/1.22.0/7b3cff25c75ab38c20165a4b64a6c21a1ec0a68e/temporal-testing-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-test-server/1.22.0/3b6ed588d67334693039d2a6e296a097ed8abc99/temporal-test-server-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-sdk/1.22.0/91f81ba7b9673a892bd89536637474310691f207/temporal-sdk-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-registry-prometheus/1.9.11/6f96d09ce7db52dcf30892dacd3126fed5d08628/micrometer-registry-prometheus-1.9.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.h2database/h2/2.1.214/d5c2005c9e3279201e12d4776c948578b16bf8b2/h2-2.1.214.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/2.7.12/6c188d92a5e2ad2f6a5360d2fc720be94e0cb19/spring-boot-starter-test-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-aop/2.7.12/334620701ca6aa67c1258c67bbe65ae75607d25a/spring-boot-starter-aop-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-jdbc/2.7.12/e9cf3b67ddd74e7e0356a0eecf56475196a5e684/spring-boot-starter-jdbc-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/2.7.12/863c30cf3a7858421d71762e1632d887cdb0866f/spring-boot-starter-json-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/2.7.12/a5f6a95e4e4a50cfaf0f14c86240d8be49637141/spring-boot-starter-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.2/6cb5991baaac48f185879e13882c26c3237d00e6/kafka_2.13-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.2/8364e63acc13b8f6df8475ec5f4ede28321049dd/kafka_2.13-3.1.2-test.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.2.18/616b1bcf7ce3e45d1a49236f8aba4993d1897048/metrics-core-4.2.18.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.1.2/8b39bfc4028147045b2ebe591af957a2a91d683f/kafka-metadata-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.14.2/2f3c71211b6ea7a978eba33574d7135d536e07fb/jackson-datatype-jdk8-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator-autoconfigure/2.7.12/18f55d22dc8d23f46f6dfd8dd352f3ae0bc8587c/spring-boot-actuator-autoconfigure-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.14.2/796518148a385b2728d44886cc0f8852eb8eeb53/jackson-datatype-jsr310-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.14.2/2b6c19b3d99dda02915515df879ab9e23fed3864/jackson-module-parameter-names-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.14.2/c6201b16c9317197e97368e6c3f696da399d5b0f/jackson-dataformat-csv-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.14.2/478ce3b018edfb16e7ce6debdfd42b9bd79b829d/jackson-module-scala_2.13-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.14.2/f804090e6399ce0cf78242db086017512dd71fcc/jackson-core-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.1.2/f3f4412c14522040ca98149ac19c3bca3a69f620/kafka-raft-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.1.2/4288869c80495c2cede53e1049f5957e4c883307/kafka-storage-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.1.2/b59e8bc21251836c057b57e7812ee90a4fe192e2/kafka-streams-test-utils-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.1.2/2782e33ae622f4158ee768a3886f179ee125bb66/kafka-streams-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.14.2/1e71fddbc80bb86f71a6345ac1e8ab8a00e7134/jackson-databind-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.14.2/a7aae9525864930723e3453ab799521fdfd9d873/jackson-annotations-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/4.5.1/f81fb60bd69b3a6e5537ae23b883326f01632a61/mockito-junit-jupiter-4.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.8.2/ddeafe92fc263f895bfb73ffeca7fd56e23c2cce/junit-jupiter-params-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.8.2/c598b4328d2f397194d11df3b1648d68d7d990e3/junit-jupiter-engine-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.8.2/4c21029217adf07e4c0d0c5e192b6bf610c94bdc/junit-jupiter-api-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.8.2/b737de09f19864bd136805c84df7999a142fec29/junit-platform-engine-1.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.8.2/32c8b8617c1342376fd5af2053da6410d8866861/junit-platform-commons-1.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.8.2/5a817b1e63f1217e5c586090c45e681281f097ad/junit-jupiter-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/2.7.12/b6590492b4d92baebb4ac7c1b5db11fc3488e347/spring-boot-starter-logging-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.17.2/17dd0fae2747d9a28c67bc9534108823d2376b46/log4j-to-slf4j-2.17.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.17.2/f42d6afa111b4dec5d2aea0fe2197240749a4ea6/log4j-api-2.17.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-core/1.9.11/c9c51ffe93b55af9deb2e77ac2fa1777b9ee1c14/micrometer-core-1.9.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/4.5.1/ed456e623e5afc6f4cee3ae58144e5c45f3b3bf/mockito-core-4.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.6.3/a6e74f826db85ff8c51c15ef0fa2ea0b462aef25/zookeeper-3.6.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.92.Final/d8e961d89a966c0cdea88105bbb2353408a41d12/netty-handler-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.92.Final/64a60fa4ba3bfe45c4f02bc9e87b0a7a89fdd71d/netty-transport-native-epoll-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.92.Final/ca149d6a9a028eafcd6b45e3884529d04628fe64/netty-transport-classes-epoll-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.92.Final/22364cca56b752abb49fb7972f0f6299b8d6be98/netty-transport-native-unix-common-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.92.Final/9496b3d59290edcc6480cbe064f33560873f7484/netty-codec-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.92.Final/e0cf483b7c04af7207c02a6ff0c861592b09c97a/netty-transport-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.92.Final/5ff1ba7ec68c20e635fc0f2b792c38c951f688d6/netty-buffer-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.92.Final/fe96e210a4e139cb043d26702792b6098b93b13f/netty-resolver-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.92.Final/66133f9ad31816a227acf3030632903cb9e4c5a2/netty-common-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_common/0.15.0/57bd1d8be9f4d965a38c6b1b35ee60358cc679fc/simpleclient_common-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient/0.15.0/144aaf1ac9361a497d98079e0db8757a95e22fc4/simpleclient-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_tracer_otel/0.15.0/53770a575d13d5aeebc7e2ebd7cc714496d7ab28/simpleclient_tracer_otel-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_tracer_otel_agent/0.15.0/9c2f1a317960110581857911ca5fd7379ba77e28/simpleclient_tracer_otel_agent-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-jpa/2.7.12/a81f0cce406074c684b0237ed0795b00f750be3c/spring-data-jpa-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-commons/2.7.12/a40381f82e9b0d02a9768b10409f1b74a062f0f4/spring-data-commons-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/5.3.27/8beabbb0cb64c448b39df790b62e342245ee7971/spring-webmvc-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/2.7.12/4568bf5fc7811f71d7e5625bc9128855965bc193/spring-boot-test-autoconfigure-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/2.7.12/5bb8661bba72f7ca353ae486ccb06285f0ed84eb/spring-boot-autoconfigure-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/2.7.12/e24936543eb9db6c37eecc8879248d51f8e7e476/spring-boot-test-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator/2.7.12/c5b5961120028a8a7db9b2d5b4f5f926fd0b0b15/spring-boot-actuator-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/2.7.12/888c3545dc3c6ca791753c7ad621a2d03f222732/spring-boot-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/5.3.27/251162aa2fe5cb374a482ae87fa6e8e8e747d72/spring-context-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/5.3.27/ecae2962d53c587fd0e405cd60dc8415d1b9e12d/spring-aop-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aspects/5.3.27/d6311337bd1eb60cb7da6f064c97c4b22a16497/spring-aspects-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/5.3.27/a51c45a8602052a2a90f7e645a47ba8df1547795/spring-web-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/5.3.27/db5b9c3d41a0493b7bcfd4117625ab83199b514c/spring-messaging-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-orm/5.3.27/80823b39c7fe25ec825e7e13cb7ac0053fada349/spring-orm-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jdbc/5.3.27/c96414a7531595d9c7d2addee132b362fa5ef65f/spring-jdbc-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/5.3.27/be6e6752ad18f0944af21c3da8987b6506e13c53/spring-tx-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/5.3.27/46e7d917551ffcc0a104fd971d1fa207b30d7761/spring-beans-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/5.3.27/84e2e80189abd5b74f2b07abde8d179a404a5b71/spring-test-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/5.3.27/6225619970e8376df5c3d777610d9d03b977063b/spring-expression-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/5.3.27/ff5e35f2d4f72d22c476ff9b7bce1de25c980ebd/spring-core-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/5.3.27/9698ea7d5361b5e3a27ed08beb7770279bd2397/spring-jcl-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hibernate/hibernate-core/5.6.15.Final/ab14b7cef1fdff654ca81923048a6034d6c7cfa7/hibernate-core-5.6.15.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/antlr/antlr/2.7.7/83cd2cd674a217ade95a4bb83a8a14f351f48bd0/antlr-2.7.7.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjweaver/1.9.7/158f5c255cd3e4408e795b79f7c3fbae9b53b7ca/aspectjweaver-1.9.7.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.22.0/c300c0c6a24559f35fa0bd3a5472dc1edcd0111e/assertj-core-3.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.12.23/d470526e8c4566c04e9ae5d3ccb62d1a7aa58986/byte-buddy-1.12.23.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.12.23/1cba11fdb72c383edacb909f79ae6870efd275e4/byte-buddy-agent-1.12.23.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml/classmate/1.5.1/3fe0bed568c62df5e89f4f174c101eab25345b6c/classmate-1.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/jaxb-runtime/2.3.8/c90a335a07c60db986f29d35b0f8ac0a18f0f989/jaxb-runtime-2.3.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/txw2/2.3.8/66e0297f1196f0d15a776e699de1b8e6ac5d44dd/txw2-2.3.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-serviceclient/1.22.0/181dcb9254407cb2deaabd8605f83881625aa1dd/temporal-serviceclient-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-services/1.54.1/c3a7eb776bf6aa5b4a0f67f9930f67accbda4dbb/grpc-services-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.protobuf/protobuf-java-util/3.22.0/749cd4fe8ab52f37bc186193802ba19f5b284647/protobuf-java-util-3.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-netty-shaded/1.54.1/fadabf30cbdb0eb2cb0bfd8c4f33dd89cc410857/grpc-netty-shaded-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-core/1.54.1/55370c1ab902c6700f8088eb82873a77478698f1/grpc-core-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.code.gson/gson/2.10.1/b3add478d4382b78ea20b1671390a858002feb6c/gson-2.10.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.zaxxer/HikariCP/4.0.3/107cbdf0db6780a065f895ae9d8fbf3bb0e1c21f/HikariCP-4.0.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.sun.activation/jakarta.activation/1.2.2/74548703f9851017ce2f556066659438019e7eb5/jakarta.activation-1.2.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/2.3.3/48e3b9cfc10752fba3521d6511f4165bea951801/jakarta.xml.bind-api-2.3.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/1.2.2/99f53adba383cb1bf7c3862844488574b559621f/jakarta.activation-api-1.2.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/2.7.12/ab5e32159703fbc429009cd5d0cffc9497d02773/spring-boot-starter-tomcat-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/1.3.5/59eb84ee0d616332ff44aba065f3888cf002cd2d/jakarta.annotation-api-1.3.5.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.persistence/jakarta.persistence-api/2.2.3/8f6ea5daedc614f07a3654a455660145286f024e/jakarta.persistence-api-2.2.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.transaction/jakarta.transaction-api/1.3.3/c4179d48720a1e87202115fbed6089bdc4195405/jakarta.transaction-api-1.3.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hibernate.common/hibernate-commons-annotations/5.1.2.Final/e59ffdbc6ad09eeb33507b39ffcf287679a498c8/hibernate-commons-annotations-5.1.2.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.4.3.Final/c4bd7e12a745c0e7f6cf98c45cdcdf482fd827ea/jboss-logging-3.4.3.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.8.0/b4ab3b7a9e425655a0ca65487bbbd6d7ddb75160/json-path-2.8.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.4.11/cc5888f14a5768f254b97bafe8b9fd29b31e872e/json-smart-2.4.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.1/6d842d0faf4cf6725c509a5e5347d319ee0431c3/jsonassert-1.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.1.2/17a50f0cdd93ddbd17d9d99fbbe60140993132ed/kafka-server-common-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.1.2/c23bc615bfd36be99a7e2edb35fa7e0503e8be9f/kafka-storage-api-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.2/6b6e2cc01cd7e772296941aca74b2fff96e7c820/kafka-clients-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.2/fc70a9fbcec3855d856e1ccd88d6e09da84da98a/kafka-clients-3.1.2-test.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.12/d4dee19148dccb177a0736eb2027bd195341da78/logback-classic-1.2.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.2.12/1d8e51a698b138065d73baefb4f94531faa323cb/logback-core-1.2.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/1.7.36/ed46d81cef9c412a88caef405b58f93a678ff2ca/jul-to-slf4j-1.7.36.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.thymeleaf/thymeleaf-spring5/3.0.15.RELEASE/7170e1bcd1588d38c139f7048ebcc262676441c3/thymeleaf-spring5-3.0.15.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.thymeleaf.extras/thymeleaf-extras-java8time/3.0.4.RELEASE/36e7175ddce36c486fff4578b5af7bb32f54f5df/thymeleaf-extras-java8time-3.0.4.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.thymeleaf/thymeleaf/3.0.15.RELEASE/13e3296a03d8a597b734d832ed8656139bf9cdd8/thymeleaf-3.0.15.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.7.8/34b47db4364d1916c78c3e26e419e8acbff57d80/jose4j-0.7.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.3/d97c3bcdcf6179e110af8ad2a64ca276843395c1/scala-logging_2.13-3.9.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.36/6c62681a2f655b49963a5983b8b0950a6120ae14/slf4j-api-1.7.36.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.30/8fde7fe2586328ac3c68db92045e1c8759125000/snakeyaml-1.30.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.3.4/4262d75536b193ea70bd3e854155462623d180a5/spring-retry-1.3.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/9.0.75/236a09fb990d3f3c15af22cc7e3cf5bce73fe5c3/tomcat-embed-websocket-9.0.75.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/9.0.75/a431d28cd877f3e45bfad7e8cc4c9ca3f4a2a206/tomcat-embed-core-9.0.75.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/9.0.75/8b97771742cbd402ad1350190f8d08b240e1793b/tomcat-embed-el-9.0.75.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.9.1/e5833662d9a1279a37da3ef6f62a1da29fcd68c4/xmlunit-core-2.9.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-opentracing-shim/1.25.0-alpha/87f4d4bb648368933f77e6432e4e9d799dce9b66/opentelemetry-opentracing-shim-1.25.0-alpha.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-semconv/1.25.0-alpha/8b64d93091d11acd546b81f6f5f221c986b2728a/opentelemetry-semconv-1.25.0-alpha.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-api/1.25.0/d30030ae2634bbbce1003f9404d964c9a7085833/opentelemetry-api-1.25.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-opentracing/1.22.0/2847d331ab018ad555a31c5951799c45b78e2c53/temporal-opentracing-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-stub/1.54.1/6f2267abcdf55223a315b1df8348c94820ba3d7a/grpc-stub-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-protobuf/1.54.1/3af888f22f0242a4b1272954166173f96db47a79/grpc-protobuf-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-protobuf-lite/1.54.1/865388468541fc52f1375f93a89864272e335cb3/grpc-protobuf-lite-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-api/1.54.1/246ee0cafff9104aba6edd5c4423d4a43216c620/grpc-api-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.guava/guava/31.1-jre/60458f877d055d0c9114d9e1a2efb737b4bc282c/guava-31.1-jre.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.uber.m3/tally-core/0.13.0/f16036272ad126c72c3730e9bc575dbf5317814a/tally-core-0.13.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.code.findbugs/jsr305/3.0.2/25ea2e8b0c338a877313bd4672d3fe056ea78f0d/jsr305-3.0.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-context/1.25.0/84e2188b9fb8aebb91915c79cc4d38567fc903a6/opentelemetry-context-1.25.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentracing/opentracing-util/0.33.0/132630f17e198a1748f23ce33597efdf4a807fb9/opentracing-util-0.33.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentracing/opentracing-noop/0.33.0/74b9950a587f53fbdb48c3f1f84f1ece8c10592/opentracing-noop-0.33.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentracing/opentracing-api/0.33.0/67336cfb9d93779c02e1fda4c87801d352720eda/opentracing-api-0.33.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-context/1.54.1/7225b68b8f61cb68014b85cacd4365e657e80b5b/grpc-context-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.guava/failureaccess/1.0.1/1dcf1de382a0bf95a3d8b0849546c88bac1292c9/failureaccess-1.0.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/b421526c5f297295adef1c886e5246c39d4ac629/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.checkerframework/checker-qual/3.12.0/d5692f0526415fcc6de94bb5bfbd3afd9dd3b3e5/checker-qual-3.12.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.errorprone/error_prone_annotations/2.18.0/89b684257096f548fa39a7df9fdaa409d4d4df91/error_prone_annotations-2.18.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.j2objc/j2objc-annotations/1.3/ba035118bc8bac37d7eff77700720999acd9986d/j2objc-annotations-1.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.1.12/6eb7552156e0d517ae80cc2247be1427c8d90452/HdrHistogram-2.1.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.latencyutils/LatencyUtils/2.0.3/769c0b82cb2421c8256300e907298a9410a2a3d3/LatencyUtils-2.0.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.cronutils/cron-utils/9.2.1/5d3738bc7a2eaa45a94a76c6e87af54a95414637/cron-utils-9.2.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.jboss/jandex/2.4.2.Final/1e1c385990b258ff1a24c801e84aebbacf70eb39/jandex-2.4.2.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.2/7fadf57620c8b8abdf7519533e5527367cb51f09/objenesis-3.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.0-4/338d83645fb93afc9e8b38a12d9d16d41d0819b3/zstd-jni-1.5.0-4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.8.4/66f0d56454509f6e36175f2331572e250e04a6cc/snappy-java-1.1.8.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.6.3/8990d19ec3db01f45f82d4011a11b085db66de05/zookeeper-jute-3.6.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.5.0/55762d3191a8d6610ef46d11e8cb70c7667342a3/audience-annotations-0.5.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.4.4/9cb262981ba1fac1f25c0e7e2b285d536ec8923b/scala-collection-compat_2.13-2.4.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.0/8ffac68615b438933fe27506e755d790a68b8bab/scala-java8-compat_2.13-1.0.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.6/bb523e56c63746a5752dece28fcd702c54fd3a11/scala-reflect-2.13.6.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.10/67c1afabaea9ba51321159e70a78515647e1b73d/scala-library-2.13.10.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.2.0/28c11eb91f9b6d8e200631d46e20a7f407f2a046/opentest4j-1.2.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.perfmark/perfmark-api/0.25.0/340a0c3d81cdcd9ecd7dc2ae00fb2633b469b157/perfmark-api-0.25.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.4.11/245ceca7bdf3190fbb977045c852d5f3c8efece1/accessors-smart-2.4.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.sun.istack/istack-commons-runtime/3.0.12/cbbe1a62b0cc6c85972e99d52aaee350153dc530/istack-commons-runtime-3.0.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/ognl/ognl/3.1.26/922d3d922b8aa40146d842114c184c8b403d2f4f/ognl-3.1.26.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.attoparser/attoparser/2.0.5.RELEASE/a93ad36df9560de3a5312c1d14f69d938099fa64/attoparser-2.0.5.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.unbescape/unbescape/1.1.6.RELEASE/7b90360afb2b860e09e8347112800d12c12b2a13/unbescape-1.1.6.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/6.22.1.1/c9cdf28e714bc93a3e7b6c57d583d3508568a606/rocksdbjni-6.22.1.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.android/annotations/4.1.1.4/a1678ba907bf92691d879fef34e1a187038f9259/annotations-4.1.1.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.codehaus.mojo/animal-sniffer-annotations/1.21/419a9acd297cb6fe6f91b982d909f2c20e9fa5c0/animal-sniffer-annotations-1.21.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.api.grpc/proto-google-common-protos/2.9.0/e4ada41aaaf6ecdedf132f44251d0d50813f7f90/proto-google-common-protos-2.9.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.protobuf/protobuf-java/3.21.7/96cfc7147192f1de72c3d7d06972155ffb7d180c/protobuf-java-3.21.7.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.3/8e6300ef51c1d801a7ed62d07cd221aca3a90640/asm-9.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_tracer_common/0.15.0/f1bac57eaf6c04e6b72a08b44a0e6569e87974a4/simpleclient_tracer_common-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.javassist/javassist/3.20.0-GA/a9cbcdfb7e9f86fbc74d3afae65f2248bfbf82a0/javassist-3.20.0-GA.jar:/Users/himanshukapoor/.gradle/wrapper/dists/gradle-8.0.1-bin/7rsvvmdp4bsd5fhm7a9cyrnzz/gradle-8.0.1/lib/plugins/junit-platform-engine-1.8.2.jar:/Users/himanshukapoor/.gradle/wrapper/dists/gradle-8.0.1-bin/7rsvvmdp4bsd5fhm7a9cyrnzz/gradle-8.0.1/lib/plugins/junit-platform-launcher-1.8.2.jar:/Users/himanshukapoor/.gradle/wrapper/dists/gradle-8.0.1-bin/7rsvvmdp4bsd5fhm7a9cyrnzz/gradle-8.0.1/lib/plugins/junit-platform-commons-1.8.2.jar
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.library.path=/Users/himanshukapoor/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.io.tmpdir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:java.compiler=&lt;NA&gt;
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.name=Mac OS X
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.arch=aarch64
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.version=13.6
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.name=himanshukapoor
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.home=/Users/himanshukapoor
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:user.dir=/Users/himanshukapoor/projects/temporal/samples-java/springboot
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.free=49MB
2023-11-17 11:05:54.876  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.max=512MB
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Server environment:os.memory.total=161MB
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.enableEagerACLCheck = false
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.digest.enabled = true
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.closeSessionTxn.enabled = true
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.flushDelay=0
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.maxWriteQueuePollTime=0
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.maxBatchSize=1000
2023-11-17 11:05:54.877  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : zookeeper.intBufferStartingSizeBytes = 1024
2023-11-17 11:05:54.878  INFO 80454 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : zookeeper.snapshot.trust.empty : false
2023-11-17 11:05:54.886  INFO 80454 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2023-11-17 11:05:54.886  INFO 80454 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2023-11-17 11:05:54.886  INFO 80454 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.snapshotSizeFactor = 0.33
2023-11-17 11:05:54.886  INFO 80454 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.commitLogCount=500
2023-11-17 11:05:54.887  INFO 80454 --- [    Test worker] o.apache.zookeeper.server.BlueThrottle   : Weighed connection throttling is disabled
2023-11-17 11:05:54.888  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : minSessionTimeout set to 1600
2023-11-17 11:05:54.888  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : maxSessionTimeout set to 16000
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : Response cache size is initialized with value 400.
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : Response cache size is initialized with value 400.
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotCapacity = 60
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotDuration = 15
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.maxDepth = 6
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.initialDelay = 5
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.delay = 5
2023-11-17 11:05:54.889  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.enabled = false
2023-11-17 11:05:54.890  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The max bytes for all large requests are set to 104857600
2023-11-17 11:05:54.890  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The large request threshold is set to -1
2023-11-17 11:05:54.890  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-6813224121839361963/version-2 snapdir /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-1491299981944647066/version-2
2023-11-17 11:05:54.894  WARN 80454 --- [    Test worker] o.a.zookeeper.server.ServerCnxnFactory   : maxCnxns is not configured, using default value 0.
2023-11-17 11:05:54.895  INFO 80454 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 20 worker threads, and 64 kB direct buffers.
2023-11-17 11:05:54.897  INFO 80454 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : binding to port /127.0.0.1:0
2023-11-17 11:05:54.904  INFO 80454 --- [    Test worker] o.a.z.server.persistence.SnapStream      : zookeeper.snapshot.compression.method = CHECKED
2023-11-17 11:05:54.904  INFO 80454 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-1491299981944647066/version-2/snapshot.0
2023-11-17 11:05:54.907  INFO 80454 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : Snapshot loaded in 9 ms, highest zxid is 0x0, digest is 1371985504
2023-11-17 11:05:54.907  INFO 80454 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-1491299981944647066/version-2/snapshot.0
2023-11-17 11:05:54.908  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Snapshot taken in 1 ms
2023-11-17 11:05:54.913  INFO 80454 --- [0 cport:62953):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2023-11-17 11:05:54.913  INFO 80454 --- [    Test worker] o.a.zookeeper.server.RequestThrottler    : zookeeper.request_throttler.shutdownTimeout = 10000
2023-11-17 11:05:55.035  INFO 80454 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:62953
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-11-17 11:05:55.044  INFO 80454 --- [    Test worker] org.apache.zookeeper.common.X509Util     : Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
2023-11-17 11:05:55.071  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : starting
2023-11-17 11:05:55.071  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : Connecting to zookeeper on 127.0.0.1:62953
2023-11-17 11:05:55.077  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:62953.
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:host.name=himanshukapoor
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.version=17.0.4
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.vendor=Homebrew
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.home=/opt/homebrew/Cellar/openjdk@17/17.0.4/libexec/openjdk.jdk/Contents/Home
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.class.path=/Users/himanshukapoor/.gradle/caches/8.0.1/workerMain/gradle-worker.jar:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/classes/java/test:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/resources/test:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/classes/java/main:/Users/himanshukapoor/projects/temporal/samples-java/springboot/build/resources/main:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-spring-boot-starter-alpha/1.22.0/a1420aa75dfe1c3ea661aff7eba4182e63f63ada/temporal-spring-boot-starter-alpha-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-spring-boot-autoconfigure-alpha/1.22.0/13f215641f46bd927a4d1ab7acd44a25e632eae1/temporal-spring-boot-autoconfigure-alpha-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-web/2.7.12/aba3f96331e2cfa316d8ec7446804721120a0552/spring-boot-starter-web-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-thymeleaf/2.7.12/2abc3b0955eb7aee9ae4a97b3c8e660b9f7846fb/spring-boot-starter-thymeleaf-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-actuator/2.7.12/f3f66262d4b856db3afbb55e4c04446b24b085d6/spring-boot-starter-actuator-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-data-jpa/2.7.12/17fdcc9bc155a6e6459190078c3a37c7fce0ced8/spring-boot-starter-data-jpa-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka/2.8.11/7068d24ea230111777ce60232fa39d4e570d4709/spring-kafka-2.8.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.kafka/spring-kafka-test/2.8.11/593e579e7671ee39c52c61ae23df451d6ad1cefd/spring-kafka-test-2.8.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-testing/1.22.0/7b3cff25c75ab38c20165a4b64a6c21a1ec0a68e/temporal-testing-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-test-server/1.22.0/3b6ed588d67334693039d2a6e296a097ed8abc99/temporal-test-server-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-sdk/1.22.0/91f81ba7b9673a892bd89536637474310691f207/temporal-sdk-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-registry-prometheus/1.9.11/6f96d09ce7db52dcf30892dacd3126fed5d08628/micrometer-registry-prometheus-1.9.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.h2database/h2/2.1.214/d5c2005c9e3279201e12d4776c948578b16bf8b2/h2-2.1.214.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-test/2.7.12/6c188d92a5e2ad2f6a5360d2fc720be94e0cb19/spring-boot-starter-test-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-aop/2.7.12/334620701ca6aa67c1258c67bbe65ae75607d25a/spring-boot-starter-aop-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-jdbc/2.7.12/e9cf3b67ddd74e7e0356a0eecf56475196a5e684/spring-boot-starter-jdbc-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-json/2.7.12/863c30cf3a7858421d71762e1632d887cdb0866f/spring-boot-starter-json-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter/2.7.12/a5f6a95e4e4a50cfaf0f14c86240d8be49637141/spring-boot-starter-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.2/6cb5991baaac48f185879e13882c26c3237d00e6/kafka_2.13-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka_2.13/3.1.2/8364e63acc13b8f6df8475ec5f4ede28321049dd/kafka_2.13-3.1.2-test.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.dropwizard.metrics/metrics-core/4.2.18/616b1bcf7ce3e45d1a49236f8aba4993d1897048/metrics-core-4.2.18.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-metadata/3.1.2/8b39bfc4028147045b2ebe591af957a2a91d683f/kafka-metadata-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jdk8/2.14.2/2f3c71211b6ea7a978eba33574d7135d536e07fb/jackson-datatype-jdk8-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator-autoconfigure/2.7.12/18f55d22dc8d23f46f6dfd8dd352f3ae0bc8587c/spring-boot-actuator-autoconfigure-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.datatype/jackson-datatype-jsr310/2.14.2/796518148a385b2728d44886cc0f8852eb8eeb53/jackson-datatype-jsr310-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-parameter-names/2.14.2/2b6c19b3d99dda02915515df879ab9e23fed3864/jackson-module-parameter-names-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.dataformat/jackson-dataformat-csv/2.14.2/c6201b16c9317197e97368e6c3f696da399d5b0f/jackson-dataformat-csv-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.module/jackson-module-scala_2.13/2.14.2/478ce3b018edfb16e7ce6debdfd42b9bd79b829d/jackson-module-scala_2.13-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-core/2.14.2/f804090e6399ce0cf78242db086017512dd71fcc/jackson-core-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-raft/3.1.2/f3f4412c14522040ca98149ac19c3bca3a69f620/kafka-raft-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage/3.1.2/4288869c80495c2cede53e1049f5957e4c883307/kafka-storage-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams-test-utils/3.1.2/b59e8bc21251836c057b57e7812ee90a4fe192e2/kafka-streams-test-utils-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-streams/3.1.2/2782e33ae622f4158ee768a3886f179ee125bb66/kafka-streams-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-databind/2.14.2/1e71fddbc80bb86f71a6345ac1e8ab8a00e7134/jackson-databind-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml.jackson.core/jackson-annotations/2.14.2/a7aae9525864930723e3453ab799521fdfd9d873/jackson-annotations-2.14.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-junit-jupiter/4.5.1/f81fb60bd69b3a6e5537ae23b883326f01632a61/mockito-junit-jupiter-4.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-params/5.8.2/ddeafe92fc263f895bfb73ffeca7fd56e23c2cce/junit-jupiter-params-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-engine/5.8.2/c598b4328d2f397194d11df3b1648d68d7d990e3/junit-jupiter-engine-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter-api/5.8.2/4c21029217adf07e4c0d0c5e192b6bf610c94bdc/junit-jupiter-api-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-engine/1.8.2/b737de09f19864bd136805c84df7999a142fec29/junit-platform-engine-1.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.platform/junit-platform-commons/1.8.2/32c8b8617c1342376fd5af2053da6410d8866861/junit-platform-commons-1.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.junit.jupiter/junit-jupiter/5.8.2/5a817b1e63f1217e5c586090c45e681281f097ad/junit-jupiter-5.8.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-logging/2.7.12/b6590492b4d92baebb4ac7c1b5db11fc3488e347/spring-boot-starter-logging-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-to-slf4j/2.17.2/17dd0fae2747d9a28c67bc9534108823d2376b46/log4j-to-slf4j-2.17.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.17.2/f42d6afa111b4dec5d2aea0fe2197240749a4ea6/log4j-api-2.17.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.micrometer/micrometer-core/1.9.11/c9c51ffe93b55af9deb2e77ac2fa1777b9ee1c14/micrometer-core-1.9.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.mockito/mockito-core/4.5.1/ed456e623e5afc6f4cee3ae58144e5c45f3b3bf/mockito-core-4.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper/3.6.3/a6e74f826db85ff8c51c15ef0fa2ea0b462aef25/zookeeper-3.6.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-handler/4.1.92.Final/d8e961d89a966c0cdea88105bbb2353408a41d12/netty-handler-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-epoll/4.1.92.Final/64a60fa4ba3bfe45c4f02bc9e87b0a7a89fdd71d/netty-transport-native-epoll-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-classes-epoll/4.1.92.Final/ca149d6a9a028eafcd6b45e3884529d04628fe64/netty-transport-classes-epoll-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport-native-unix-common/4.1.92.Final/22364cca56b752abb49fb7972f0f6299b8d6be98/netty-transport-native-unix-common-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-codec/4.1.92.Final/9496b3d59290edcc6480cbe064f33560873f7484/netty-codec-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-transport/4.1.92.Final/e0cf483b7c04af7207c02a6ff0c861592b09c97a/netty-transport-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-buffer/4.1.92.Final/5ff1ba7ec68c20e635fc0f2b792c38c951f688d6/netty-buffer-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-resolver/4.1.92.Final/fe96e210a4e139cb043d26702792b6098b93b13f/netty-resolver-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.netty/netty-common/4.1.92.Final/66133f9ad31816a227acf3030632903cb9e4c5a2/netty-common-4.1.92.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_common/0.15.0/57bd1d8be9f4d965a38c6b1b35ee60358cc679fc/simpleclient_common-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient/0.15.0/144aaf1ac9361a497d98079e0db8757a95e22fc4/simpleclient-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_tracer_otel/0.15.0/53770a575d13d5aeebc7e2ebd7cc714496d7ab28/simpleclient_tracer_otel-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_tracer_otel_agent/0.15.0/9c2f1a317960110581857911ca5fd7379ba77e28/simpleclient_tracer_otel_agent-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-jpa/2.7.12/a81f0cce406074c684b0237ed0795b00f750be3c/spring-data-jpa-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.data/spring-data-commons/2.7.12/a40381f82e9b0d02a9768b10409f1b74a062f0f4/spring-data-commons-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-webmvc/5.3.27/8beabbb0cb64c448b39df790b62e342245ee7971/spring-webmvc-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test-autoconfigure/2.7.12/4568bf5fc7811f71d7e5625bc9128855965bc193/spring-boot-test-autoconfigure-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-autoconfigure/2.7.12/5bb8661bba72f7ca353ae486ccb06285f0ed84eb/spring-boot-autoconfigure-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-test/2.7.12/e24936543eb9db6c37eecc8879248d51f8e7e476/spring-boot-test-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-actuator/2.7.12/c5b5961120028a8a7db9b2d5b4f5f926fd0b0b15/spring-boot-actuator-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot/2.7.12/888c3545dc3c6ca791753c7ad621a2d03f222732/spring-boot-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-context/5.3.27/251162aa2fe5cb374a482ae87fa6e8e8e747d72/spring-context-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aop/5.3.27/ecae2962d53c587fd0e405cd60dc8415d1b9e12d/spring-aop-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-aspects/5.3.27/d6311337bd1eb60cb7da6f064c97c4b22a16497/spring-aspects-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-web/5.3.27/a51c45a8602052a2a90f7e645a47ba8df1547795/spring-web-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-messaging/5.3.27/db5b9c3d41a0493b7bcfd4117625ab83199b514c/spring-messaging-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-orm/5.3.27/80823b39c7fe25ec825e7e13cb7ac0053fada349/spring-orm-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jdbc/5.3.27/c96414a7531595d9c7d2addee132b362fa5ef65f/spring-jdbc-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-tx/5.3.27/be6e6752ad18f0944af21c3da8987b6506e13c53/spring-tx-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-beans/5.3.27/46e7d917551ffcc0a104fd971d1fa207b30d7761/spring-beans-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-test/5.3.27/84e2e80189abd5b74f2b07abde8d179a404a5b71/spring-test-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-expression/5.3.27/6225619970e8376df5c3d777610d9d03b977063b/spring-expression-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-core/5.3.27/ff5e35f2d4f72d22c476ff9b7bce1de25c980ebd/spring-core-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework/spring-jcl/5.3.27/9698ea7d5361b5e3a27ed08beb7770279bd2397/spring-jcl-5.3.27.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hibernate/hibernate-core/5.6.15.Final/ab14b7cef1fdff654ca81923048a6034d6c7cfa7/hibernate-core-5.6.15.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/antlr/antlr/2.7.7/83cd2cd674a217ade95a4bb83a8a14f351f48bd0/antlr-2.7.7.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.aspectj/aspectjweaver/1.9.7/158f5c255cd3e4408e795b79f7c3fbae9b53b7ca/aspectjweaver-1.9.7.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.assertj/assertj-core/3.22.0/c300c0c6a24559f35fa0bd3a5472dc1edcd0111e/assertj-core-3.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy/1.12.23/d470526e8c4566c04e9ae5d3ccb62d1a7aa58986/byte-buddy-1.12.23.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.bytebuddy/byte-buddy-agent/1.12.23/1cba11fdb72c383edacb909f79ae6870efd275e4/byte-buddy-agent-1.12.23.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.fasterxml/classmate/1.5.1/3fe0bed568c62df5e89f4f174c101eab25345b6c/classmate-1.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/jaxb-runtime/2.3.8/c90a335a07c60db986f29d35b0f8ac0a18f0f989/jaxb-runtime-2.3.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.glassfish.jaxb/txw2/2.3.8/66e0297f1196f0d15a776e699de1b8e6ac5d44dd/txw2-2.3.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-serviceclient/1.22.0/181dcb9254407cb2deaabd8605f83881625aa1dd/temporal-serviceclient-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-services/1.54.1/c3a7eb776bf6aa5b4a0f67f9930f67accbda4dbb/grpc-services-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.protobuf/protobuf-java-util/3.22.0/749cd4fe8ab52f37bc186193802ba19f5b284647/protobuf-java-util-3.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-netty-shaded/1.54.1/fadabf30cbdb0eb2cb0bfd8c4f33dd89cc410857/grpc-netty-shaded-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-core/1.54.1/55370c1ab902c6700f8088eb82873a77478698f1/grpc-core-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.code.gson/gson/2.10.1/b3add478d4382b78ea20b1671390a858002feb6c/gson-2.10.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hamcrest/hamcrest/2.2/1820c0968dba3a11a1b30669bb1f01978a91dedc/hamcrest-2.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.zaxxer/HikariCP/4.0.3/107cbdf0db6780a065f895ae9d8fbf3bb0e1c21f/HikariCP-4.0.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.sun.activation/jakarta.activation/1.2.2/74548703f9851017ce2f556066659438019e7eb5/jakarta.activation-1.2.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.xml.bind/jakarta.xml.bind-api/2.3.3/48e3b9cfc10752fba3521d6511f4165bea951801/jakarta.xml.bind-api-2.3.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.activation/jakarta.activation-api/1.2.2/99f53adba383cb1bf7c3862844488574b559621f/jakarta.activation-api-1.2.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.boot/spring-boot-starter-tomcat/2.7.12/ab5e32159703fbc429009cd5d0cffc9497d02773/spring-boot-starter-tomcat-2.7.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.annotation/jakarta.annotation-api/1.3.5/59eb84ee0d616332ff44aba065f3888cf002cd2d/jakarta.annotation-api-1.3.5.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.persistence/jakarta.persistence-api/2.2.3/8f6ea5daedc614f07a3654a455660145286f024e/jakarta.persistence-api-2.2.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/jakarta.transaction/jakarta.transaction-api/1.3.3/c4179d48720a1e87202115fbed6089bdc4195405/jakarta.transaction-api-1.3.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hibernate.common/hibernate-commons-annotations/5.1.2.Final/e59ffdbc6ad09eeb33507b39ffcf287679a498c8/hibernate-commons-annotations-5.1.2.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.jboss.logging/jboss-logging/3.4.3.Final/c4bd7e12a745c0e7f6cf98c45cdcdf482fd827ea/jboss-logging-3.4.3.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.jayway.jsonpath/json-path/2.8.0/b4ab3b7a9e425655a0ca65487bbbd6d7ddb75160/json-path-2.8.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.minidev/json-smart/2.4.11/cc5888f14a5768f254b97bafe8b9fd29b31e872e/json-smart-2.4.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.skyscreamer/jsonassert/1.5.1/6d842d0faf4cf6725c509a5e5347d319ee0431c3/jsonassert-1.5.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-server-common/3.1.2/17a50f0cdd93ddbd17d9d99fbbe60140993132ed/kafka-server-common-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-storage-api/3.1.2/c23bc615bfd36be99a7e2edb35fa7e0503e8be9f/kafka-storage-api-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.2/6b6e2cc01cd7e772296941aca74b2fff96e7c820/kafka-clients-3.1.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.kafka/kafka-clients/3.1.2/fc70a9fbcec3855d856e1ccd88d6e09da84da98a/kafka-clients-3.1.2-test.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-classic/1.2.12/d4dee19148dccb177a0736eb2027bd195341da78/logback-classic-1.2.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/ch.qos.logback/logback-core/1.2.12/1d8e51a698b138065d73baefb4f94531faa323cb/logback-core-1.2.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.slf4j/jul-to-slf4j/1.7.36/ed46d81cef9c412a88caef405b58f93a678ff2ca/jul-to-slf4j-1.7.36.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.thymeleaf/thymeleaf-spring5/3.0.15.RELEASE/7170e1bcd1588d38c139f7048ebcc262676441c3/thymeleaf-spring5-3.0.15.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.thymeleaf.extras/thymeleaf-extras-java8time/3.0.4.RELEASE/36e7175ddce36c486fff4578b5af7bb32f54f5df/thymeleaf-extras-java8time-3.0.4.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.thymeleaf/thymeleaf/3.0.15.RELEASE/13e3296a03d8a597b734d832ed8656139bf9cdd8/thymeleaf-3.0.15.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.bitbucket.b_c/jose4j/0.7.8/34b47db4364d1916c78c3e26e419e8acbff57d80/jose4j-0.7.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.yammer.metrics/metrics-core/2.2.0/f82c035cfa786d3cbec362c38c22a5f5b1bc8724/metrics-core-2.2.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.typesafe.scala-logging/scala-logging_2.13/3.9.3/d97c3bcdcf6179e110af8ad2a64ca276843395c1/scala-logging_2.13-3.9.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.slf4j/slf4j-api/1.7.36/6c62681a2f655b49963a5983b8b0950a6120ae14/slf4j-api-1.7.36.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.yaml/snakeyaml/1.30/8fde7fe2586328ac3c68db92045e1c8759125000/snakeyaml-1.30.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.springframework.retry/spring-retry/1.3.4/4262d75536b193ea70bd3e854155462623d180a5/spring-retry-1.3.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-websocket/9.0.75/236a09fb990d3f3c15af22cc7e3cf5bce73fe5c3/tomcat-embed-websocket-9.0.75.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-core/9.0.75/a431d28cd877f3e45bfad7e8cc4c9ca3f4a2a206/tomcat-embed-core-9.0.75.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.tomcat.embed/tomcat-embed-el/9.0.75/8b97771742cbd402ad1350190f8d08b240e1793b/tomcat-embed-el-9.0.75.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.xmlunit/xmlunit-core/2.9.1/e5833662d9a1279a37da3ef6f62a1da29fcd68c4/xmlunit-core-2.9.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-opentracing-shim/1.25.0-alpha/87f4d4bb648368933f77e6432e4e9d799dce9b66/opentelemetry-opentracing-shim-1.25.0-alpha.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-semconv/1.25.0-alpha/8b64d93091d11acd546b81f6f5f221c986b2728a/opentelemetry-semconv-1.25.0-alpha.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-api/1.25.0/d30030ae2634bbbce1003f9404d964c9a7085833/opentelemetry-api-1.25.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.temporal/temporal-opentracing/1.22.0/2847d331ab018ad555a31c5951799c45b78e2c53/temporal-opentracing-1.22.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-stub/1.54.1/6f2267abcdf55223a315b1df8348c94820ba3d7a/grpc-stub-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-protobuf/1.54.1/3af888f22f0242a4b1272954166173f96db47a79/grpc-protobuf-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-protobuf-lite/1.54.1/865388468541fc52f1375f93a89864272e335cb3/grpc-protobuf-lite-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-api/1.54.1/246ee0cafff9104aba6edd5c4423d4a43216c620/grpc-api-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.guava/guava/31.1-jre/60458f877d055d0c9114d9e1a2efb737b4bc282c/guava-31.1-jre.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.uber.m3/tally-core/0.13.0/f16036272ad126c72c3730e9bc575dbf5317814a/tally-core-0.13.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.code.findbugs/jsr305/3.0.2/25ea2e8b0c338a877313bd4672d3fe056ea78f0d/jsr305-3.0.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentelemetry/opentelemetry-context/1.25.0/84e2188b9fb8aebb91915c79cc4d38567fc903a6/opentelemetry-context-1.25.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentracing/opentracing-util/0.33.0/132630f17e198a1748f23ce33597efdf4a807fb9/opentracing-util-0.33.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentracing/opentracing-noop/0.33.0/74b9950a587f53fbdb48c3f1f84f1ece8c10592/opentracing-noop-0.33.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.opentracing/opentracing-api/0.33.0/67336cfb9d93779c02e1fda4c87801d352720eda/opentracing-api-0.33.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.grpc/grpc-context/1.54.1/7225b68b8f61cb68014b85cacd4365e657e80b5b/grpc-context-1.54.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.guava/failureaccess/1.0.1/1dcf1de382a0bf95a3d8b0849546c88bac1292c9/failureaccess-1.0.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.guava/listenablefuture/9999.0-empty-to-avoid-conflict-with-guava/b421526c5f297295adef1c886e5246c39d4ac629/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.checkerframework/checker-qual/3.12.0/d5692f0526415fcc6de94bb5bfbd3afd9dd3b3e5/checker-qual-3.12.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.errorprone/error_prone_annotations/2.18.0/89b684257096f548fa39a7df9fdaa409d4d4df91/error_prone_annotations-2.18.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.j2objc/j2objc-annotations/1.3/ba035118bc8bac37d7eff77700720999acd9986d/j2objc-annotations-1.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.hdrhistogram/HdrHistogram/2.1.12/6eb7552156e0d517ae80cc2247be1427c8d90452/HdrHistogram-2.1.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.latencyutils/LatencyUtils/2.0.3/769c0b82cb2421c8256300e907298a9410a2a3d3/LatencyUtils-2.0.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.cronutils/cron-utils/9.2.1/5d3738bc7a2eaa45a94a76c6e87af54a95414637/cron-utils-9.2.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.jboss/jandex/2.4.2.Final/1e1c385990b258ff1a24c801e84aebbacf70eb39/jandex-2.4.2.Final.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.objenesis/objenesis/3.2/7fadf57620c8b8abdf7519533e5527367cb51f09/objenesis-3.2.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.vaadin.external.google/android-json/0.0.20131108.vaadin1/fa26d351fe62a6a17f5cda1287c1c6110dec413f/android-json-0.0.20131108.vaadin1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.github.luben/zstd-jni/1.5.0-4/338d83645fb93afc9e8b38a12d9d16d41d0819b3/zstd-jni-1.5.0-4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.lz4/lz4-java/1.8.0/4b986a99445e49ea5fbf5d149c4b63f6ed6c6780/lz4-java-1.8.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.xerial.snappy/snappy-java/1.1.8.4/66f0d56454509f6e36175f2331572e250e04a6cc/snappy-java-1.1.8.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.zookeeper/zookeeper-jute/3.6.3/8990d19ec3db01f45f82d4011a11b085db66de05/zookeeper-jute-3.6.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.apache.yetus/audience-annotations/0.5.0/55762d3191a8d6610ef46d11e8cb70c7667342a3/audience-annotations-0.5.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.sourceforge.argparse4j/argparse4j/0.7.0/6f0621d0c3888de39e0f06d01f37ba53a798e657/argparse4j-0.7.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.sf.jopt-simple/jopt-simple/5.0.4/4fdac2fbe92dfad86aa6e9301736f6b4342a3f5c/jopt-simple-5.0.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-collection-compat_2.13/2.4.4/9cb262981ba1fac1f25c0e7e2b285d536ec8923b/scala-collection-compat_2.13-2.4.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang.modules/scala-java8-compat_2.13/1.0.0/8ffac68615b438933fe27506e755d790a68b8bab/scala-java8-compat_2.13-1.0.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-reflect/2.13.6/bb523e56c63746a5752dece28fcd702c54fd3a11/scala-reflect-2.13.6.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/commons-cli/commons-cli/1.4/c51c00206bb913cd8612b24abd9fa98ae89719b1/commons-cli-1.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.scala-lang/scala-library/2.13.10/67c1afabaea9ba51321159e70a78515647e1b73d/scala-library-2.13.10.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.opentest4j/opentest4j/1.2.0/28c11eb91f9b6d8e200631d46e20a7f407f2a046/opentest4j-1.2.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.perfmark/perfmark-api/0.25.0/340a0c3d81cdcd9ecd7dc2ae00fb2633b469b157/perfmark-api-0.25.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/net.minidev/accessors-smart/2.4.11/245ceca7bdf3190fbb977045c852d5f3c8efece1/accessors-smart-2.4.11.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.sun.istack/istack-commons-runtime/3.0.12/cbbe1a62b0cc6c85972e99d52aaee350153dc530/istack-commons-runtime-3.0.12.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/ognl/ognl/3.1.26/922d3d922b8aa40146d842114c184c8b403d2f4f/ognl-3.1.26.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.attoparser/attoparser/2.0.5.RELEASE/a93ad36df9560de3a5312c1d14f69d938099fa64/attoparser-2.0.5.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.unbescape/unbescape/1.1.6.RELEASE/7b90360afb2b860e09e8347112800d12c12b2a13/unbescape-1.1.6.RELEASE.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.thoughtworks.paranamer/paranamer/2.8/619eba74c19ccf1da8ebec97a2d7f8ba05773dd6/paranamer-2.8.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.rocksdb/rocksdbjni/6.22.1.1/c9cdf28e714bc93a3e7b6c57d583d3508568a606/rocksdbjni-6.22.1.1.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.android/annotations/4.1.1.4/a1678ba907bf92691d879fef34e1a187038f9259/annotations-4.1.1.4.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.codehaus.mojo/animal-sniffer-annotations/1.21/419a9acd297cb6fe6f91b982d909f2c20e9fa5c0/animal-sniffer-annotations-1.21.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.api.grpc/proto-google-common-protos/2.9.0/e4ada41aaaf6ecdedf132f44251d0d50813f7f90/proto-google-common-protos-2.9.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/com.google.protobuf/protobuf-java/3.21.7/96cfc7147192f1de72c3d7d06972155ffb7d180c/protobuf-java-3.21.7.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.ow2.asm/asm/9.3/8e6300ef51c1d801a7ed62d07cd221aca3a90640/asm-9.3.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/io.prometheus/simpleclient_tracer_common/0.15.0/f1bac57eaf6c04e6b72a08b44a0e6569e87974a4/simpleclient_tracer_common-0.15.0.jar:/Users/himanshukapoor/.gradle/caches/modules-2/files-2.1/org.javassist/javassist/3.20.0-GA/a9cbcdfb7e9f86fbc74d3afae65f2248bfbf82a0/javassist-3.20.0-GA.jar:/Users/himanshukapoor/.gradle/wrapper/dists/gradle-8.0.1-bin/7rsvvmdp4bsd5fhm7a9cyrnzz/gradle-8.0.1/lib/plugins/junit-platform-engine-1.8.2.jar:/Users/himanshukapoor/.gradle/wrapper/dists/gradle-8.0.1-bin/7rsvvmdp4bsd5fhm7a9cyrnzz/gradle-8.0.1/lib/plugins/junit-platform-launcher-1.8.2.jar:/Users/himanshukapoor/.gradle/wrapper/dists/gradle-8.0.1-bin/7rsvvmdp4bsd5fhm7a9cyrnzz/gradle-8.0.1/lib/plugins/junit-platform-commons-1.8.2.jar
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.library.path=/Users/himanshukapoor/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.io.tmpdir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:java.compiler=&lt;NA&gt;
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.name=Mac OS X
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.arch=aarch64
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.version=13.6
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.name=himanshukapoor
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.home=/Users/himanshukapoor
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:user.dir=/Users/himanshukapoor/projects/temporal/samples-java/springboot
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.free=93MB
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.max=512MB
2023-11-17 11:05:55.079  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Client environment:os.memory.total=161MB
2023-11-17 11:05:55.080  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Initiating client connection, connectString=127.0.0.1:62953 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@2667a238
2023-11-17 11:05:55.081  INFO 80454 --- [    Test worker] org.apache.zookeeper.ClientCnxnSocket    : jute.maxbuffer value is 4194304 Bytes
2023-11-17 11:05:55.083  INFO 80454 --- [    Test worker] org.apache.zookeeper.ClientCnxn          : zookeeper.request.timeout value is 0. feature enabled=false
2023-11-17 11:05:55.084  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Waiting until connected.
2023-11-17 11:05:55.086  INFO 80454 --- [27.0.0.1:62953)] org.apache.zookeeper.ClientCnxn          : Opening socket connection to server localhost/127.0.0.1:62953.
2023-11-17 11:05:55.086  INFO 80454 --- [27.0.0.1:62953)] org.apache.zookeeper.ClientCnxn          : SASL config status: Will not attempt to authenticate using SASL (unknown error)
2023-11-17 11:05:55.088  INFO 80454 --- [27.0.0.1:62953)] org.apache.zookeeper.ClientCnxn          : Socket connection established, initiating session, client: /127.0.0.1:62954, server: localhost/127.0.0.1:62953
2023-11-17 11:05:55.092  INFO 80454 --- [   SyncThread:0] o.a.z.server.persistence.FileTxnLog      : Creating new log file: log.1
2023-11-17 11:05:55.094  INFO 80454 --- [   SyncThread:0] o.a.zookeeper.audit.ZKAuditProvider      : ZooKeeper audit is disabled.
2023-11-17 11:05:55.095  INFO 80454 --- [27.0.0.1:62953)] org.apache.zookeeper.ClientCnxn          : Session establishment complete on server localhost/127.0.0.1:62953, session id = 0x1003407a4060000, negotiated timeout = 16000
2023-11-17 11:05:55.097  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Connected.
2023-11-17 11:05:55.129  INFO 80454 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Starting
2023-11-17 11:05:55.134  INFO 80454 --- [-process-thread] k.server.FinalizedFeatureChangeListener  : Feature ZK node at path: /feature does not exist
2023-11-17 11:05:55.134  INFO 80454 --- [-process-thread] kafka.server.FinalizedFeatureCache       : Cleared cache
2023-11-17 11:05:55.141  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : Cluster ID = FnQCiTDIRq6xdi6bUVv17w
2023-11-17 11:05:55.142  WARN 80454 --- [    Test worker] kafka.server.BrokerMetadataCheckpoint    : No meta.properties file under dir /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/meta.properties
2023-11-17 11:05:55.156  INFO 80454 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:62953
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-11-17 11:05:55.160  INFO 80454 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:0
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:62953
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-11-17 11:05:55.173  INFO 80454 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Starting
2023-11-17 11:05:55.173  INFO 80454 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Starting
2023-11-17 11:05:55.173  INFO 80454 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Starting
2023-11-17 11:05:55.174  INFO 80454 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Starting
2023-11-17 11:05:55.185  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Loading logs from log dirs ArraySeq(/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926)
2023-11-17 11:05:55.187  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Attempting recovery for all logs in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926 since no clean shutdown file was found
2023-11-17 11:05:55.189  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Loaded 0 logs in 4ms.
2023-11-17 11:05:55.189  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Starting log cleanup with a period of 300000 ms.
2023-11-17 11:05:55.190  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Starting log flusher with a default period of 9223372036854775807 ms.
2023-11-17 11:05:55.195  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : Starting the log cleaner
2023-11-17 11:05:55.201  INFO 80454 --- [leaner-thread-0] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Starting
2023-11-17 11:05:55.308  INFO 80454 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2023-11-17 11:05:55.363  INFO 80454 --- [    Test worker] kafka.network.ConnectionQuotas           : Updated connection-accept-rate max connection creation rate to 2147483647
2023-11-17 11:05:55.365  INFO 80454 --- [    Test worker] kafka.network.Acceptor                   : Awaiting socket connections on localhost:62959.
2023-11-17 11:05:55.376  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-11-17 11:05:55.379  INFO 80454 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2023-11-17 11:05:55.386  INFO 80454 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Starting
2023-11-17 11:05:55.387  INFO 80454 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Starting
2023-11-17 11:05:55.387  INFO 80454 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Starting
2023-11-17 11:05:55.388  INFO 80454 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Starting
2023-11-17 11:05:55.395  INFO 80454 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Starting
2023-11-17 11:05:55.403  INFO 80454 --- [    Test worker] kafka.zk.KafkaZkClient                   : Creating /brokers/ids/0 (is it secure? false)
2023-11-17 11:05:55.409  INFO 80454 --- [    Test worker] kafka.zk.KafkaZkClient                   : Stat of the created znode at /brokers/ids/0 is: 25,25,1700215555406,1700215555406,1,0,0,72114801459200000,204,0,25

2023-11-17 11:05:55.409  INFO 80454 --- [    Test worker] kafka.zk.KafkaZkClient                   : Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:62959, czxid (broker epoch): 25
2023-11-17 11:05:55.436  INFO 80454 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Starting
2023-11-17 11:05:55.438  INFO 80454 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Starting
2023-11-17 11:05:55.440  INFO 80454 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Starting
2023-11-17 11:05:55.440  INFO 80454 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Starting
2023-11-17 11:05:55.443  INFO 80454 --- [er-event-thread] kafka.zk.KafkaZkClient                   : Successfully created /controller_epoch with initial epoch 0
2023-11-17 11:05:55.445  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2023-11-17 11:05:55.447  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{})
2023-11-17 11:05:55.447  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Starting up.
2023-11-17 11:05:55.448  INFO 80454 --- [ker-EventThread] k.server.FinalizedFeatureChangeListener  : Feature ZK node created at path: /feature
2023-11-17 11:05:55.449  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Startup complete.
2023-11-17 11:05:55.458  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Registering handlers
2023-11-17 11:05:55.458  INFO 80454 --- [-process-thread] kafka.server.FinalizedFeatureCache       : Updated cache from existing &lt;empty&gt; to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2023-11-17 11:05:55.460  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Starting up.
2023-11-17 11:05:55.460  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting log dir event notifications
2023-11-17 11:05:55.461  INFO 80454 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Starting
2023-11-17 11:05:55.461  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Startup complete.
2023-11-17 11:05:55.461  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting isr change notifications
2023-11-17 11:05:55.462  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing controller context
2023-11-17 11:05:55.468  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initialized broker epochs cache: HashMap(0 -&gt; 25)
2023-11-17 11:05:55.469  INFO 80454 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Starting
2023-11-17 11:05:55.474  INFO 80454 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Starting
2023-11-17 11:05:55.475  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently active brokers in the cluster: Set(0)
2023-11-17 11:05:55.475  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2023-11-17 11:05:55.475  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Current list of topics in the cluster: HashSet()
2023-11-17 11:05:55.475  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Fetching topic deletions in progress
2023-11-17 11:05:55.476  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics to be deleted: 
2023-11-17 11:05:55.476  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics ineligible for deletion: 
2023-11-17 11:05:55.476  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing topic deletion manager
2023-11-17 11:05:55.477  INFO 80454 --- [er-event-thread] kafka.controller.TopicDeletionManager    : [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2023-11-17 11:05:55.477  INFO 80454 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Starting
2023-11-17 11:05:55.478  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Sending update metadata request
2023-11-17 11:05:55.479  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2023-11-17 11:05:55.481  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2023-11-17 11:05:55.482  INFO 80454 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Initializing replica state
2023-11-17 11:05:55.482  INFO 80454 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2023-11-17 11:05:55.483  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-11-17 11:05:55.483  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2023-11-17 11:05:55.483  INFO 80454 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2023-11-17 11:05:55.484  INFO 80454 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Initializing partition state
2023-11-17 11:05:55.484  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:55.484  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:55.484  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215555483
2023-11-17 11:05:55.484  INFO 80454 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Triggering online partition state changes
2023-11-17 11:05:55.484  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] started
2023-11-17 11:05:55.485  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Ready to serve as the new controller with epoch 1
2023-11-17 11:05:55.485  INFO 80454 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Controller 0 connected to localhost:62959 (id: 0 rack: null) for sending state change requests
2023-11-17 11:05:55.487  INFO 80454 --- [    Test worker] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [127.0.0.1:62959]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-11-17 11:05:55.487  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions undergoing preferred replica election: 
2023-11-17 11:05:55.487  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions that completed preferred replica election: 
2023-11-17 11:05:55.487  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2023-11-17 11:05:55.487  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Resuming preferred replica election for partitions: 
2023-11-17 11:05:55.488  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2023-11-17 11:05:55.493  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting the controller scheduler
2023-11-17 11:05:55.500  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:55.500  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:55.500  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215555500
2023-11-17 11:05:55.503  INFO 80454 --- [| adminclient-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.admin.client for adminclient-1 unregistered
2023-11-17 11:05:55.504  INFO 80454 --- [| adminclient-1] o.a.k.c.a.i.AdminMetadataManager         : [AdminClient clientId=adminclient-1] Metadata update failed

org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata

2023-11-17 11:05:55.504  INFO 80454 --- [| adminclient-1] o.a.k.clients.admin.KafkaAdminClient     : [AdminClient clientId=adminclient-1] Timed out 1 remaining operation(s) during close.
2023-11-17 11:05:55.505  INFO 80454 --- [| adminclient-1] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:05:55.505  INFO 80454 --- [| adminclient-1] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:05:55.506  INFO 80454 --- [| adminclient-1] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:05:55.507  INFO 80454 --- [    Test worker] i.t.samples.springboot.KafkaSampleTest   : Starting KafkaSampleTest using Java 17.0.4 on himanshukapoor with PID 80454 (started by himanshukapoor in /Users/himanshukapoor/projects/temporal/samples-java/springboot)
2023-11-17 11:05:55.507  INFO 80454 --- [    Test worker] i.t.samples.springboot.KafkaSampleTest   : No active profile set, falling back to 1 default profile: &quot;default&quot;
2023-11-17 11:05:55.514  INFO 80454 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:62959 (id: 0 rack: null)
2023-11-17 11:05:55.581  INFO 80454 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:62959 (id: 0 rack: null)
2023-11-17 11:05:55.594  INFO 80454 --- [    Test worker] .s.d.r.c.RepositoryConfigurationDelegate : Bootstrapping Spring Data JPA repositories in DEFAULT mode.
2023-11-17 11:05:55.599  INFO 80454 --- [    Test worker] .s.d.r.c.RepositoryConfigurationDelegate : Finished Spring Data repository scanning in 5 ms. Found 1 JPA repository interfaces.
2023-11-17 11:05:55.665  INFO 80454 --- [    Test worker] o.hibernate.jpa.internal.util.LogHelper  : HHH000204: Processing PersistenceUnitInfo [name: default]
2023-11-17 11:05:55.667  INFO 80454 --- [    Test worker] com.zaxxer.hikari.HikariDataSource       : HikariPool-3 - Starting...
2023-11-17 11:05:55.667  INFO 80454 --- [    Test worker] com.zaxxer.hikari.HikariDataSource       : HikariPool-3 - Start completed.
2023-11-17 11:05:55.667  INFO 80454 --- [    Test worker] org.hibernate.dialect.Dialect            : HHH000400: Using dialect: org.hibernate.dialect.H2Dialect
2023-11-17 11:05:55.677  INFO 80454 --- [    Test worker] o.h.e.t.j.p.i.JtaPlatformInitiator       : HHH000490: Using JtaPlatform implementation: [org.hibernate.engine.transaction.jta.platform.internal.NoJtaPlatform]
2023-11-17 11:05:55.677  INFO 80454 --- [    Test worker] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit 'default'
2023-11-17 11:05:55.706  INFO 80454 --- [    Test worker] i.t.s.b.a.TestServerAutoConfiguration    : `TemporalOptionsCustomizer&lt;WorkflowServiceStubsOptions.Builder&gt;` bean is ignored for test environment
2023-11-17 11:05:55.707  INFO 80454 --- [    Test worker] i.t.s.WorkflowServiceStubsImpl           : Created WorkflowServiceStubs for channel: ManagedChannelOrphanWrapper{delegate=ManagedChannelImpl{logId=12, target=directaddress:///eddef80c-0ed1-4ae8-a7a1-636a70703881}}
2023-11-17 11:05:55.707  INFO 80454 --- [    Test worker] i.t.serviceclient.TestServiceStubsImpl   : Created TestServiceStubs for channel: ManagedChannelOrphanWrapper{delegate=ManagedChannelImpl{logId=12, target=directaddress:///eddef80c-0ed1-4ae8-a7a1-636a70703881}}
2023-11-17 11:05:55.707  INFO 80454 --- [    Test worker] i.t.s.OperatorServiceStubsImpl           : Created OperatorServiceStubs for channel: ManagedChannelOrphanWrapper{delegate=ManagedChannelImpl{logId=12, target=directaddress:///eddef80c-0ed1-4ae8-a7a1-636a70703881}}
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : zookeeper.snapshot.trust.empty : false
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] o.a.z.server.watch.WatchManagerFactory   : Using org.apache.zookeeper.server.watch.WatchManager as watch manager
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.snapshotSizeFactor = 0.33
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : zookeeper.commitLogCount=500
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : minSessionTimeout set to 1600
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : maxSessionTimeout set to 16000
2023-11-17 11:05:55.764  INFO 80454 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : Response cache size is initialized with value 400.
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.apache.zookeeper.server.ResponseCache  : Response cache size is initialized with value 400.
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotCapacity = 60
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.slotDuration = 15
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.maxDepth = 6
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.initialDelay = 5
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.delay = 5
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.s.u.RequestPathMetricsCollector    : zookeeper.pathStats.enabled = false
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The max bytes for all large requests are set to 104857600
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : The large request threshold is set to -1
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Created server with tickTime 800 minSessionTimeout 1600 maxSessionTimeout 16000 clientPortListenBacklog -1 datadir /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-16023706563487210871/version-2 snapdir /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-11189413590296200435/version-2
2023-11-17 11:05:55.765  WARN 80454 --- [    Test worker] o.a.zookeeper.server.ServerCnxnFactory   : maxCnxns is not configured, using default value 0.
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : Configuring NIO connection handler with 10s sessionless connection timeout, 2 selector thread(s), 20 worker threads, and 64 kB direct buffers.
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.server.NIOServerCnxnFactory        : binding to port /127.0.0.1:0
2023-11-17 11:05:55.765  INFO 80454 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-11189413590296200435/version-2/snapshot.0
2023-11-17 11:05:55.766  INFO 80454 --- [    Test worker] org.apache.zookeeper.server.ZKDatabase   : Snapshot loaded in 1 ms, highest zxid is 0x0, digest is 1371985504
2023-11-17 11:05:55.766  INFO 80454 --- [    Test worker] o.a.z.server.persistence.FileTxnSnapLog  : Snapshotting: 0x0 to /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/kafka-11189413590296200435/version-2/snapshot.0
2023-11-17 11:05:55.766  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : Snapshot taken in 0 ms
2023-11-17 11:05:55.766  INFO 80454 --- [0 cport:62962):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor (sid:0) started, reconfigEnabled=false
2023-11-17 11:05:55.767  INFO 80454 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.ccbf7c11-79ae-4084-96ad-ec0f57d5b0bf9908760113913840995
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:62962
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-11-17 11:05:55.768  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : starting
2023-11-17 11:05:55.768  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : Connecting to zookeeper on 127.0.0.1:62962
2023-11-17 11:05:55.768  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Initializing a new session to 127.0.0.1:62962.
2023-11-17 11:05:55.768  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Initiating client connection, connectString=127.0.0.1:62962 sessionTimeout=18000 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@721fda59
2023-11-17 11:05:55.768  INFO 80454 --- [    Test worker] org.apache.zookeeper.ClientCnxnSocket    : jute.maxbuffer value is 4194304 Bytes
2023-11-17 11:05:55.768  INFO 80454 --- [    Test worker] org.apache.zookeeper.ClientCnxn          : zookeeper.request.timeout value is 0. feature enabled=false
2023-11-17 11:05:55.769  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Waiting until connected.
2023-11-17 11:05:55.770  INFO 80454 --- [27.0.0.1:62962)] org.apache.zookeeper.ClientCnxn          : Opening socket connection to server localhost/127.0.0.1:62962.
2023-11-17 11:05:55.770  INFO 80454 --- [27.0.0.1:62962)] org.apache.zookeeper.ClientCnxn          : SASL config status: Will not attempt to authenticate using SASL (unknown error)
2023-11-17 11:05:55.771  INFO 80454 --- [27.0.0.1:62962)] org.apache.zookeeper.ClientCnxn          : Socket connection established, initiating session, client: /127.0.0.1:62963, server: localhost/127.0.0.1:62962
2023-11-17 11:05:55.771  INFO 80454 --- [   SyncThread:0] o.a.z.server.persistence.FileTxnLog      : Creating new log file: log.1
2023-11-17 11:05:55.772  INFO 80454 --- [27.0.0.1:62962)] org.apache.zookeeper.ClientCnxn          : Session establishment complete on server localhost/127.0.0.1:62962, session id = 0x1003407a75f0000, negotiated timeout = 16000
2023-11-17 11:05:55.772  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Connected.
2023-11-17 11:05:55.784  INFO 80454 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Starting
2023-11-17 11:05:55.785  INFO 80454 --- [-process-thread] k.server.FinalizedFeatureChangeListener  : Feature ZK node at path: /feature does not exist
2023-11-17 11:05:55.785  INFO 80454 --- [-process-thread] kafka.server.FinalizedFeatureCache       : Cleared cache
2023-11-17 11:05:55.787  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : Cluster ID = gPH7sK6AQhWFyOkQXlzbFg
2023-11-17 11:05:55.787  WARN 80454 --- [    Test worker] kafka.server.BrokerMetadataCheckpoint    : No meta.properties file under dir /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.ccbf7c11-79ae-4084-96ad-ec0f57d5b0bf9908760113913840995/meta.properties
2023-11-17 11:05:55.789  INFO 80454 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.ccbf7c11-79ae-4084-96ad-ec0f57d5b0bf9908760113913840995
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:62962
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-11-17 11:05:55.791  INFO 80454 --- [    Test worker] kafka.server.KafkaConfig                 : KafkaConfig values: 
	advertised.listeners = null
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	broker.session.timeout.ms = 9000
	client.quota.callback.class = null
	compression.type = producer
	connection.failed.authentication.delay.ms = 100
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = false
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 100
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 1000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 0
	group.max.session.timeout.ms = 1800000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = null
	inter.broker.protocol.version = 3.1-IV0
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = PLAINTEXT://localhost:9092
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 2097152
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.ccbf7c11-79ae-4084-96ad-ec0f57d5b0bf9908760113913840995
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 3.0-IV1
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 1000
	max.connection.creation.rate = 2147483647
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	node.id = 0
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 2
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 5
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.window.num = 11
	quota.window.size.seconds = 1
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 9223372036854775807
	replica.lag.time.max.ms = 30000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 1000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	sasl.server.max.receive.size = 524288
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = null
	zookeeper.connect = 127.0.0.1:62962
	zookeeper.connection.timeout.ms = 10000
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 18000
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = false
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = null
	zookeeper.ssl.truststore.password = null
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

2023-11-17 11:05:55.797  INFO 80454 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Starting
2023-11-17 11:05:55.797  INFO 80454 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Starting
2023-11-17 11:05:55.797  INFO 80454 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Starting
2023-11-17 11:05:55.797  INFO 80454 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Starting
2023-11-17 11:05:55.798  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Loading logs from log dirs ArraySeq(/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.ccbf7c11-79ae-4084-96ad-ec0f57d5b0bf9908760113913840995)
2023-11-17 11:05:55.798  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Attempting recovery for all logs in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.ccbf7c11-79ae-4084-96ad-ec0f57d5b0bf9908760113913840995 since no clean shutdown file was found
2023-11-17 11:05:55.799  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Loaded 0 logs in 1ms.
2023-11-17 11:05:55.799  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Starting log cleanup with a period of 300000 ms.
2023-11-17 11:05:55.799  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Starting log flusher with a default period of 9223372036854775807 ms.
2023-11-17 11:05:55.800  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : Starting the log cleaner
2023-11-17 11:05:55.800  INFO 80454 --- [leaner-thread-0] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Starting
2023-11-17 11:05:55.801  INFO 80454 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
2023-11-17 11:05:55.806  INFO 80454 --- [    Test worker] kafka.network.ConnectionQuotas           : Updated connection-accept-rate max connection creation rate to 2147483647
2023-11-17 11:05:55.806  INFO 80454 --- [    Test worker] kafka.network.Acceptor                   : Awaiting socket connections on localhost:9092.
2023-11-17 11:05:55.807  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT)
2023-11-17 11:05:55.808  INFO 80454 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Starting
2023-11-17 11:05:55.808  INFO 80454 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Starting
2023-11-17 11:05:55.809  INFO 80454 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Starting
2023-11-17 11:05:55.809  INFO 80454 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Starting
2023-11-17 11:05:55.809  INFO 80454 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Starting
2023-11-17 11:05:55.810  INFO 80454 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Starting
2023-11-17 11:05:55.811  INFO 80454 --- [    Test worker] kafka.zk.KafkaZkClient                   : Creating /brokers/ids/0 (is it secure? false)
2023-11-17 11:05:55.812  INFO 80454 --- [    Test worker] kafka.zk.KafkaZkClient                   : Stat of the created znode at /brokers/ids/0 is: 25,25,1700215555811,1700215555811,1,0,0,72114801515364352,202,0,25

2023-11-17 11:05:55.812  INFO 80454 --- [    Test worker] kafka.zk.KafkaZkClient                   : Registered broker 0 at path /brokers/ids/0 with addresses: PLAINTEXT://localhost:9092, czxid (broker epoch): 25
2023-11-17 11:05:55.819  INFO 80454 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Starting
2023-11-17 11:05:55.820  INFO 80454 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Starting
2023-11-17 11:05:55.820  INFO 80454 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Starting
2023-11-17 11:05:55.820  INFO 80454 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Starting
2023-11-17 11:05:55.820  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Starting up.
2023-11-17 11:05:55.821  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Startup complete.
2023-11-17 11:05:55.821  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Starting up.
2023-11-17 11:05:55.821  INFO 80454 --- [er-event-thread] kafka.zk.KafkaZkClient                   : Successfully created /controller_epoch with initial epoch 0
2023-11-17 11:05:55.822  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Startup complete.
2023-11-17 11:05:55.822  INFO 80454 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Starting
2023-11-17 11:05:55.822  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] 0 successfully elected as the controller. Epoch incremented to 1 and epoch zk version is now 1
2023-11-17 11:05:55.822  INFO 80454 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Starting
2023-11-17 11:05:55.823  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Creating FeatureZNode at path: /feature with contents: FeatureZNode(Enabled,Features{})
2023-11-17 11:05:55.823  INFO 80454 --- [ker-EventThread] k.server.FinalizedFeatureChangeListener  : Feature ZK node created at path: /feature
2023-11-17 11:05:55.823  INFO 80454 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Starting
2023-11-17 11:05:55.824  INFO 80454 --- [-process-thread] kafka.server.FinalizedFeatureCache       : Updated cache from existing &lt;empty&gt; to latest FinalizedFeaturesAndEpoch(features=Features{}, epoch=0).
2023-11-17 11:05:55.824  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Registering handlers
2023-11-17 11:05:55.825  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting log dir event notifications
2023-11-17 11:05:55.825  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Deleting isr change notifications
2023-11-17 11:05:55.826  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing controller context
2023-11-17 11:05:55.826  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
2023-11-17 11:05:55.826  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(PLAINTEXT)
2023-11-17 11:05:55.826  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
2023-11-17 11:05:55.826  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:55.826  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:55.826  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215555826
2023-11-17 11:05:55.827  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initialized broker epochs cache: HashMap(0 -&gt; 25)
2023-11-17 11:05:55.828  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently active brokers in the cluster: Set(0)
2023-11-17 11:05:55.828  INFO 80454 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Starting
2023-11-17 11:05:55.828  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Currently shutting brokers in the cluster: HashSet()
2023-11-17 11:05:55.828  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Current list of topics in the cluster: HashSet()
2023-11-17 11:05:55.828  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Fetching topic deletions in progress
2023-11-17 11:05:55.828  WARN 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Error registering AppInfo mbean

javax.management.InstanceAlreadyExistsException: kafka.server:type=app-info,id=0
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436) ~[na:na]
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1865) ~[na:na]
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:960) ~[na:na]
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:895) ~[na:na]
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320) ~[na:na]
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:523) ~[na:na]
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64) ~[kafka-clients-3.1.2.jar:na]
	at kafka.server.KafkaServer.startup(KafkaServer.scala:449) ~[kafka_2.13-3.1.2.jar:na]
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:177) ~[kafka_2.13-3.1.2-test.jar:na]
	at kafka.utils.TestUtils$.createServer(TestUtils.scala:172) ~[kafka_2.13-3.1.2-test.jar:na]
	at kafka.utils.TestUtils.createServer(TestUtils.scala) ~[kafka_2.13-3.1.2-test.jar:na]
	at org.springframework.kafka.test.EmbeddedKafkaBroker.afterPropertiesSet(EmbeddedKafkaBroker.java:369) ~[spring-kafka-test-2.8.11.jar:2.8.11]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1863) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1800) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:620) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:542) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.AbstractBeanFactory.lambda$doGetBean$0(AbstractBeanFactory.java:335) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:234) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:333) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:208) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:955) ~[spring-beans-5.3.27.jar:5.3.27]
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:920) ~[spring-context-5.3.27.jar:5.3.27]
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:583) ~[spring-context-5.3.27.jar:5.3.27]
	at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:731) ~[spring-boot-2.7.12.jar:2.7.12]
	at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:408) ~[spring-boot-2.7.12.jar:2.7.12]
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:307) ~[spring-boot-2.7.12.jar:2.7.12]
	at org.springframework.boot.test.context.SpringBootContextLoader.loadContext(SpringBootContextLoader.java:136) ~[spring-boot-test-2.7.12.jar:2.7.12]
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContextInternal(DefaultCacheAwareContextLoaderDelegate.java:141) ~[spring-test-5.3.27.jar:5.3.27]
	at org.springframework.test.context.cache.DefaultCacheAwareContextLoaderDelegate.loadContext(DefaultCacheAwareContextLoaderDelegate.java:90) ~[spring-test-5.3.27.jar:5.3.27]
	at org.springframework.test.context.support.DefaultTestContext.getApplicationContext(DefaultTestContext.java:124) ~[spring-test-5.3.27.jar:5.3.27]
	at org.springframework.test.context.web.ServletTestExecutionListener.setUpRequestContextIfNecessary(ServletTestExecutionListener.java:190) ~[spring-test-5.3.27.jar:5.3.27]
	at org.springframework.test.context.web.ServletTestExecutionListener.prepareTestInstance(ServletTestExecutionListener.java:132) ~[spring-test-5.3.27.jar:5.3.27]
	at org.springframework.test.context.TestContextManager.prepareTestInstance(TestContextManager.java:248) ~[spring-test-5.3.27.jar:5.3.27]
	at org.springframework.test.context.junit.jupiter.SpringExtension.postProcessTestInstance(SpringExtension.java:138) ~[spring-test-5.3.27.jar:5.3.27]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$8(ClassBasedTestDescriptor.java:363) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.executeAndMaskThrowable(ClassBasedTestDescriptor.java:368) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeTestInstancePostProcessors$9(ClassBasedTestDescriptor.java:363) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197) ~[na:na]
	at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179) ~[na:na]
	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) ~[na:na]
	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) ~[na:na]
	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) ~[na:na]
	at java.base/java.util.stream.StreamSpliterators$WrappingSpliterator.forEachRemaining(StreamSpliterators.java:310) ~[na:na]
	at java.base/java.util.stream.Streams$ConcatSpliterator.forEachRemaining(Streams.java:735) ~[na:na]
	at java.base/java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:762) ~[na:na]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeTestInstancePostProcessors(ClassBasedTestDescriptor.java:362) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$instantiateAndPostProcessTestInstance$6(ClassBasedTestDescriptor.java:283) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.instantiateAndPostProcessTestInstance(ClassBasedTestDescriptor.java:282) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$4(ClassBasedTestDescriptor.java:272) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at java.base/java.util.Optional.orElseGet(Optional.java:364) ~[na:na]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$testInstancesProvider$5(ClassBasedTestDescriptor.java:271) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.jupiter.engine.execution.TestInstancesProvider.getTestInstances(TestInstancesProvider.java:31) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$before$2(ClassBasedTestDescriptor.java:197) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:196) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:80) ~[junit-jupiter-engine-5.8.2.jar:5.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:148) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1511) ~[na:na]
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54) ~[junit-platform-engine-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86) ~[junit-platform-launcher-1.8.2.jar:1.8.2]
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:110) ~[na:na]
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:90) ~[na:na]
	at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:85) ~[na:na]
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:62) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
	at java.base/java.lang.reflect.Method.invoke(Method.java:568) ~[na:na]
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36) ~[na:na]
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24) ~[na:na]
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33) ~[na:na]
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94) ~[na:na]
	at jdk.proxy1/jdk.proxy1.$Proxy2.stop(Unknown Source) ~[na:na]
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:193) ~[na:na]
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129) ~[na:na]
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100) ~[na:na]
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60) ~[na:na]
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56) ~[na:na]
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:113) ~[na:na]
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:65) ~[na:na]
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69) ~[gradle-worker.jar:na]
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74) ~[gradle-worker.jar:na]

2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics to be deleted: 
2023-11-17 11:05:55.829  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] started
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] List of topics ineligible for deletion: 
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Initializing topic deletion manager
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.TopicDeletionManager    : [Topic Deletion Manager 0] Initializing manager with initial deletions: Set(), initial ineligible deletions: HashSet()
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Sending update metadata request
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 0 partitions
2023-11-17 11:05:55.829  INFO 80454 --- [    Test worker] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [127.0.0.1:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Initializing replica state
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering online replica state changes
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Triggering offline replica state changes
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Initializing partition state
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Triggering online partition state changes
2023-11-17 11:05:55.829  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Ready to serve as the new controller with epoch 1
2023-11-17 11:05:55.830  INFO 80454 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Controller 0 connected to localhost:9092 (id: 0 rack: null) for sending state change requests
2023-11-17 11:05:55.830  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:55.830  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:55.830  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215555830
2023-11-17 11:05:55.830  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions undergoing preferred replica election: 
2023-11-17 11:05:55.830  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Partitions that completed preferred replica election: 
2023-11-17 11:05:55.830  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Skipping preferred replica election for partitions due to topic deletion: 
2023-11-17 11:05:55.830  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Resuming preferred replica election for partitions: 
2023-11-17 11:05:55.830  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting replica leader election (PREFERRED) for partitions  triggered by ZkTriggered
2023-11-17 11:05:55.830  INFO 80454 --- [| adminclient-2] o.a.kafka.common.utils.AppInfoParser     : App info kafka.admin.client for adminclient-2 unregistered
2023-11-17 11:05:55.830  INFO 80454 --- [| adminclient-2] o.a.k.c.a.i.AdminMetadataManager         : [AdminClient clientId=adminclient-2] Metadata update failed

org.apache.kafka.common.errors.TimeoutException: Timed out waiting to send the call. Call: fetchMetadata

2023-11-17 11:05:55.830  INFO 80454 --- [| adminclient-2] o.a.k.clients.admin.KafkaAdminClient     : [AdminClient clientId=adminclient-2] Timed out 1 remaining operation(s) during close.
2023-11-17 11:05:55.830  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Starting the controller scheduler
2023-11-17 11:05:55.831  INFO 80454 --- [| adminclient-2] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:05:55.831  INFO 80454 --- [| adminclient-2] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:05:55.831  INFO 80454 --- [| adminclient-2] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:05:55.858  WARN 80454 --- [    Test worker] JpaBaseConfiguration$JpaWebConfiguration : spring.jpa.open-in-view is enabled by default. Therefore, database queries may be performed during view rendering. Explicitly configure spring.jpa.open-in-view to disable this warning
2023-11-17 11:05:55.896  INFO 80454 --- [    Test worker] o.s.b.a.w.s.WelcomePageHandlerMapping    : Adding welcome page template: index
2023-11-17 11:05:55.902  INFO 80454 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker localhost:9092 (id: 0 rack: null)
2023-11-17 11:05:55.909  INFO 80454 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Recorded new controller, from now on will use broker localhost:9092 (id: 0 rack: null)
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Creating a worker with default settings for a task queue 'UpdateSampleTaskQueue' caused by an auto-discovered workflow class class io.temporal.samples.springboot.update.PurchaseWorkflowImpl
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered workflow class class io.temporal.samples.springboot.update.PurchaseWorkflowImpl on a worker with a task queue 'UpdateSampleTaskQueue'
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Creating a worker with default settings for a task queue 'KafkaSampleTaskQueue' caused by an auto-discovered workflow class class io.temporal.samples.springboot.kafka.MessageWorkflowImpl
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered workflow class class io.temporal.samples.springboot.kafka.MessageWorkflowImpl on a worker with a task queue 'KafkaSampleTaskQueue'
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Creating a worker with default settings for a task queue 'HelloSampleTaskQueue' caused by an auto-discovered workflow class class io.temporal.samples.springboot.hello.HelloWorkflowImpl
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered workflow class class io.temporal.samples.springboot.hello.HelloWorkflowImpl on a worker with a task queue 'HelloSampleTaskQueue'
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Creating a worker with default settings for a task queue 'CustomizeTaskQueue' caused by an auto-discovered workflow class class io.temporal.samples.springboot.customize.CustomizeWorkflowImpl
2023-11-17 11:05:55.940  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered workflow class class io.temporal.samples.springboot.customize.CustomizeWorkflowImpl on a worker with a task queue 'CustomizeTaskQueue'
2023-11-17 11:05:55.941  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered activity bean 'customizeActivityImpl' of class class io.temporal.samples.springboot.customize.CustomizeActivityImpl on a worker with a task queue 'CustomizeTaskQueue'
2023-11-17 11:05:55.941  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered activity bean 'helloActivityImpl' of class class io.temporal.samples.springboot.hello.HelloActivityImpl on a worker with a task queue 'HelloSampleTaskQueue'
2023-11-17 11:05:55.941  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered activity bean 'kafkaActivityImpl' of class class io.temporal.samples.springboot.kafka.KafkaActivityImpl on a worker with a task queue 'KafkaSampleTaskQueue'
2023-11-17 11:05:55.941  INFO 80454 --- [    Test worker] i.t.s.b.a.template.WorkersTemplate       : Registering auto-discovered activity bean 'purchaseActivitiesImpl' of class class io.temporal.samples.springboot.update.PurchaseActivitiesImpl on a worker with a task queue 'UpdateSampleTaskQueue'
2023-11-17 11:05:56.017  INFO 80454 --- [    Test worker] o.s.b.a.e.web.EndpointLinksResolver      : Exposing 1 endpoint(s) beneath base path '/actuator'
2023-11-17 11:05:56.030  INFO 80454 --- [    Test worker] o.a.k.clients.admin.AdminClientConfig    : AdminClientConfig values: 
	bootstrap.servers = [127.0.0.1:62959]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

2023-11-17 11:05:56.031  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:56.031  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:56.031  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215556031
2023-11-17 11:05:56.063  INFO 80454 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic samples-test-topic with configuration {} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0))
2023-11-17 11:05:56.070  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(samples-test-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(samples-test-topic,Some(ZCIuzbw5T4W_nbQJTB4KKA),Map(samples-test-topic-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2023-11-17 11:05:56.070  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for samples-test-topic-0
2023-11-17 11:05:56.071  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition samples-test-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2023-11-17 11:05:56.071  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.073  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.078  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition samples-test-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2023-11-17 11:05:56.079  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2023-11-17 11:05:56.080  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2023-11-17 11:05:56.080  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.084  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 0 for 1 partitions
2023-11-17 11:05:56.094  INFO 80454 --- [quest-handler-0] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(samples-test-topic-0)
2023-11-17 11:05:56.094  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2023-11-17 11:05:56.126  INFO 80454 --- [quest-handler-0] kafka.log.UnifiedLog$                    : [LogLoader partition=samples-test-topic-0, dir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926] Loading producer state till offset 0 with message format version 2
2023-11-17 11:05:56.131  INFO 80454 --- [quest-handler-0] kafka.log.LogManager                     : Created log for partition samples-test-topic-0 in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/samples-test-topic-0 with properties {}
2023-11-17 11:05:56.132  INFO 80454 --- [quest-handler-0] kafka.cluster.Partition                  : [Partition samples-test-topic-0 broker=0] No checkpointed highwatermark is found for partition samples-test-topic-0
2023-11-17 11:05:56.132  INFO 80454 --- [quest-handler-0] kafka.cluster.Partition                  : [Partition samples-test-topic-0 broker=0] Log loaded for partition samples-test-topic-0 with initial high watermark 0
2023-11-17 11:05:56.134  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Leader samples-test-topic-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2023-11-17 11:05:56.146  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 63ms correlationId 1 from controller 0 for 1 partitions
2023-11-17 11:05:56.149  INFO 80454 --- [quest-handler-1] state.change.logger                      : [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 2
2023-11-17 11:05:56.151  INFO 80454 --- [| adminclient-3] o.a.kafka.common.utils.AppInfoParser     : App info kafka.admin.client for adminclient-3 unregistered
2023-11-17 11:05:56.152  INFO 80454 --- [| adminclient-3] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:05:56.152  INFO 80454 --- [| adminclient-3] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:05:56.152  INFO 80454 --- [| adminclient-3] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:05:56.177  INFO 80454 --- [    Test worker] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:62959]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-samples-test-id-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = samples-test-id
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2023-11-17 11:05:56.191  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:56.192  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:56.192  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215556191
2023-11-17 11:05:56.197  INFO 80454 --- [    Test worker] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Subscribed to topic(s): samples-test-topic
2023-11-17 11:05:56.202  INFO 80454 --- [    Test worker] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [127.0.0.1:62959]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-samples-topic-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = samples-topic
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2023-11-17 11:05:56.203  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:56.203  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:56.203  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215556203
2023-11-17 11:05:56.205  INFO 80454 --- [    Test worker] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Subscribed to topic(s): samples-topic
2023-11-17 11:05:56.208  INFO 80454 --- [s-test-id-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Resetting the last seen epoch of partition samples-test-topic-0 to 0 since the associated topicId changed from null to ZCIuzbw5T4W_nbQJTB4KKA
2023-11-17 11:05:56.208  INFO 80454 --- [quest-handler-4] kafka.zk.AdminZkClient                   : Creating topic samples-topic with configuration {} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0))
2023-11-17 11:05:56.209  INFO 80454 --- [    Test worker] i.t.samples.springboot.KafkaSampleTest   : Started KafkaSampleTest in 1.422 seconds (JVM running for 9.189)
2023-11-17 11:05:56.209  INFO 80454 --- [s-test-id-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Cluster ID: FnQCiTDIRq6xdi6bUVv17w
2023-11-17 11:05:56.210  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : start: Poller{name=Workflow Poller taskQueue=&quot;UpdateSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.210  INFO 80454 --- [quest-handler-6] kafka.zk.AdminZkClient                   : Creating topic __consumer_offsets with configuration {compression.type=producer, cleanup.policy=compact, segment.bytes=104857600} and initial partition assignment HashMap(0 -&gt; ArrayBuffer(0), 1 -&gt; ArrayBuffer(0), 2 -&gt; ArrayBuffer(0), 3 -&gt; ArrayBuffer(0), 4 -&gt; ArrayBuffer(0))
2023-11-17 11:05:56.211  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : start: Poller{name=Activity Poller taskQueue=&quot;UpdateSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.211  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : start: Poller{name=Workflow Poller taskQueue=&quot;CustomizeTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.212  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : start: Poller{name=Workflow Poller taskQueue=&quot;HelloSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.212  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(samples-topic)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(samples-topic,Some(tEX4NR9CTe6IlDtuANvHTQ),Map(samples-topic-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2023-11-17 11:05:56.212  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for samples-topic-0
2023-11-17 11:05:56.212  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition samples-topic-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2023-11-17 11:05:56.212  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.213  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.213  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : start: Poller{name=Activity Poller taskQueue=&quot;HelloSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.213  WARN 80454 --- [les-topic-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Error while fetching metadata with correlation id 2 : {samples-topic=LEADER_NOT_AVAILABLE}
2023-11-17 11:05:56.213  INFO 80454 --- [les-topic-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Cluster ID: FnQCiTDIRq6xdi6bUVv17w
2023-11-17 11:05:56.213  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : start: Poller{name=Workflow Poller taskQueue=&quot;KafkaSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.214  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : start: Poller{name=Activity Poller taskQueue=&quot;KafkaSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.215  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition samples-topic-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2023-11-17 11:05:56.215  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 1 become-leader and 0 become-follower partitions
2023-11-17 11:05:56.215  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 1 partitions
2023-11-17 11:05:56.215  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.216  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 0 for 1 partitions
2023-11-17 11:05:56.216  INFO 80454 --- [quest-handler-0] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(samples-topic-0)
2023-11-17 11:05:56.216  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 0 epoch 1 as part of the become-leader transition for 1 partitions
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New topics: [Set(__consumer_offsets)], deleted topics: [HashSet()], new partition replica assignment [Set(TopicIdReplicaAssignment(__consumer_offsets,Some(nQh8Nv49QSeYIZUq1cWZUQ),HashMap(__consumer_offsets-4 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-3 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-2 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-0 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=), __consumer_offsets-1 -&gt; ReplicaAssignment(replicas=0, addingReplicas=, removingReplicas=))))]
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] New partition creation callback for __consumer_offsets-4,__consumer_offsets-3,__consumer_offsets-2,__consumer_offsets-0,__consumer_offsets-1
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 state from NonExistentPartition to NewPartition with assigned replicas 0
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 state from NonExistentPartition to NewPartition with assigned replicas 0
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 state from NonExistentPartition to NewPartition with assigned replicas 0
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 state from NonExistentPartition to NewPartition with assigned replicas 0
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 state from NonExistentPartition to NewPartition with assigned replicas 0
2023-11-17 11:05:56.217  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.218  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.218  INFO 80454 --- [quest-handler-0] kafka.log.UnifiedLog$                    : [LogLoader partition=samples-topic-0, dir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926] Loading producer state till offset 0 with message format version 2
2023-11-17 11:05:56.219  INFO 80454 --- [quest-handler-0] kafka.log.LogManager                     : Created log for partition samples-topic-0 in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/samples-topic-0 with properties {}
2023-11-17 11:05:56.219  INFO 80454 --- [quest-handler-0] kafka.cluster.Partition                  : [Partition samples-topic-0 broker=0] No checkpointed highwatermark is found for partition samples-topic-0
2023-11-17 11:05:56.219  INFO 80454 --- [quest-handler-0] kafka.cluster.Partition                  : [Partition samples-topic-0 broker=0] Log loaded for partition samples-topic-0 with initial high watermark 0
2023-11-17 11:05:56.219  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Leader samples-topic-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2023-11-17 11:05:56.224  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-4 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2023-11-17 11:05:56.224  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-3 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2023-11-17 11:05:56.224  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-2 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2023-11-17 11:05:56.224  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-0 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2023-11-17 11:05:56.224  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Changed partition __consumer_offsets-1 from NewPartition to OnlinePartition with state LeaderAndIsr(leader=0, leaderEpoch=0, isr=List(0), zkVersion=0)
2023-11-17 11:05:56.224  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending LeaderAndIsr request to broker 0 with 5 become-leader and 0 become-follower partitions
2023-11-17 11:05:56.224  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet(0) for 5 partitions
2023-11-17 11:05:56.225  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:56.228  INFO 80454 --- [ce=&quot;default&quot;: 1] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:62959]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2023-11-17 11:05:56.229  INFO 80454 --- [quest-handler-0] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 13ms correlationId 3 from controller 0 for 1 partitions
2023-11-17 11:05:56.229  INFO 80454 --- [quest-handler-1] state.change.logger                      : [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 4
2023-11-17 11:05:56.230  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 0 for 5 partitions
2023-11-17 11:05:56.231  INFO 80454 --- [quest-handler-2] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-4, __consumer_offsets-3, __consumer_offsets-2, __consumer_offsets-0, __consumer_offsets-1)
2023-11-17 11:05:56.231  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 0 epoch 1 as part of the become-leader transition for 5 partitions
2023-11-17 11:05:56.232  INFO 80454 --- [ce=&quot;default&quot;: 1] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-1] Instantiated an idempotent producer.
2023-11-17 11:05:56.235  INFO 80454 --- [quest-handler-2] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-3, dir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926] Loading producer state till offset 0 with message format version 2
2023-11-17 11:05:56.235  INFO 80454 --- [quest-handler-2] kafka.log.LogManager                     : Created log for partition __consumer_offsets-3 in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type=&quot;producer&quot;, segment.bytes=104857600}
2023-11-17 11:05:56.235  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
2023-11-17 11:05:56.235  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
2023-11-17 11:05:56.235  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2023-11-17 11:05:56.239  INFO 80454 --- [ce=&quot;default&quot;: 1] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.1.2
2023-11-17 11:05:56.239  INFO 80454 --- [ce=&quot;default&quot;: 1] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: f8c67dc3ae0a3265
2023-11-17 11:05:56.239  INFO 80454 --- [ce=&quot;default&quot;: 1] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1700215556239
2023-11-17 11:05:56.243  INFO 80454 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Resetting the last seen epoch of partition samples-test-topic-0 to 0 since the associated topicId changed from null to ZCIuzbw5T4W_nbQJTB4KKA
2023-11-17 11:05:56.243  INFO 80454 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : [Producer clientId=producer-1] Cluster ID: FnQCiTDIRq6xdi6bUVv17w
2023-11-17 11:05:56.245  INFO 80454 --- [quest-handler-2] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-2, dir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926] Loading producer state till offset 0 with message format version 2
2023-11-17 11:05:56.245  INFO 80454 --- [quest-handler-2] kafka.log.LogManager                     : Created log for partition __consumer_offsets-2 in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type=&quot;producer&quot;, segment.bytes=104857600}
2023-11-17 11:05:56.245  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
2023-11-17 11:05:56.245  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
2023-11-17 11:05:56.245  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2023-11-17 11:05:56.255  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=0, producerIdLen=1000} by writing to Zk with path version 1
2023-11-17 11:05:56.257  INFO 80454 --- [ad | producer-1] o.a.k.c.p.internals.TransactionManager   : [Producer clientId=producer-1] ProducerId set to 0 with epoch 0
2023-11-17 11:05:56.258  INFO 80454 --- [quest-handler-2] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-4, dir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926] Loading producer state till offset 0 with message format version 2
2023-11-17 11:05:56.258  INFO 80454 --- [quest-handler-2] kafka.log.LogManager                     : Created log for partition __consumer_offsets-4 in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type=&quot;producer&quot;, segment.bytes=104857600}
2023-11-17 11:05:56.258  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
2023-11-17 11:05:56.258  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
2023-11-17 11:05:56.258  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2023-11-17 11:05:56.269  INFO 80454 --- [quest-handler-2] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-1, dir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926] Loading producer state till offset 0 with message format version 2
2023-11-17 11:05:56.269  INFO 80454 --- [quest-handler-2] kafka.log.LogManager                     : Created log for partition __consumer_offsets-1 in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type=&quot;producer&quot;, segment.bytes=104857600}
2023-11-17 11:05:56.269  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
2023-11-17 11:05:56.269  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
2023-11-17 11:05:56.269  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2023-11-17 11:05:56.278  INFO 80454 --- [quest-handler-2] kafka.log.UnifiedLog$                    : [LogLoader partition=__consumer_offsets-0, dir=/var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926] Loading producer state till offset 0 with message format version 2
2023-11-17 11:05:56.278  INFO 80454 --- [quest-handler-2] kafka.log.LogManager                     : Created log for partition __consumer_offsets-0 in /var/folders/29/6kyvhbz576q07s92wsf1_flh0000gp/T/spring.kafka.67af0636-ec03-4dac-b0df-4b1ef8a1319f15737633079906937926/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type=&quot;producer&quot;, segment.bytes=104857600}
2023-11-17 11:05:56.278  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
2023-11-17 11:05:56.278  INFO 80454 --- [quest-handler-2] kafka.cluster.Partition                  : [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
2023-11-17 11:05:56.278  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Leader __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR [0] addingReplicas [] removingReplicas []. Previous leader epoch was -1.
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 3 in epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 4 in epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 1 in epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
2023-11-17 11:05:56.289  INFO 80454 --- [quest-handler-2] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Elected as the group coordinator for partition 0 in epoch 0
2023-11-17 11:05:56.290  INFO 80454 --- [quest-handler-2] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0
2023-11-17 11:05:56.290  INFO 80454 --- [quest-handler-2] state.change.logger                      : [Broker id=0] Finished LeaderAndIsr request in 60ms correlationId 5 from controller 0 for 5 partitions
2023-11-17 11:05:56.290  INFO 80454 --- [quest-handler-5] state.change.logger                      : [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 0 epoch 1 with correlation id 6
2023-11-17 11:05:56.292  INFO 80454 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-3 in 2 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler.
2023-11-17 11:05:56.292  INFO 80454 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2023-11-17 11:05:56.292  INFO 80454 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-4 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2023-11-17 11:05:56.292  INFO 80454 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-1 in 3 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
2023-11-17 11:05:56.292  INFO 80454 --- [adata-manager-0] k.c.group.GroupMetadataManager           : [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-0 in 2 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler.
2023-11-17 11:05:56.312  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Discovered group coordinator localhost:62959 (id: 2147483647 rack: null)
2023-11-17 11:05:56.313  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] (Re-)joining group
2023-11-17 11:05:56.316  INFO 80454 --- [les-topic-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Resetting the last seen epoch of partition samples-topic-0 to 0 since the associated topicId changed from null to tEX4NR9CTe6IlDtuANvHTQ
2023-11-17 11:05:56.316  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Discovered group coordinator localhost:62959 (id: 2147483647 rack: null)
2023-11-17 11:05:56.317  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] (Re-)joining group
2023-11-17 11:05:56.322  INFO 80454 --- [quest-handler-1] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Dynamic member with unknown member id joins group samples-topic in Empty state. Created a new member id consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2 and request the member to rejoin with this id.
2023-11-17 11:05:56.322  INFO 80454 --- [quest-handler-0] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Dynamic member with unknown member id joins group samples-test-id in Empty state. Created a new member id consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d and request the member to rejoin with this id.
2023-11-17 11:05:56.323  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Request joining group due to: need to re-join with the given member-id
2023-11-17 11:05:56.323  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Request joining group due to: need to re-join with the given member-id
2023-11-17 11:05:56.324  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] (Re-)joining group
2023-11-17 11:05:56.324  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] (Re-)joining group
2023-11-17 11:05:56.326  INFO 80454 --- [quest-handler-3] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group samples-test-id in state PreparingRebalance with old generation 0 (__consumer_offsets-4) (reason: Adding new member consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d with group instance id None)
2023-11-17 11:05:56.326  INFO 80454 --- [quest-handler-2] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group samples-topic in state PreparingRebalance with old generation 0 (__consumer_offsets-3) (reason: Adding new member consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2 with group instance id None)
2023-11-17 11:05:56.328  INFO 80454 --- [cutor-Rebalance] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Stabilized group samples-test-id generation 1 (__consumer_offsets-4) with 1 members
2023-11-17 11:05:56.329  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Successfully joined group with generation Generation{generationId=1, memberId='consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d', protocol='range'}
2023-11-17 11:05:56.329  INFO 80454 --- [cutor-Rebalance] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Stabilized group samples-topic generation 1 (__consumer_offsets-3) with 1 members
2023-11-17 11:05:56.329  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Successfully joined group with generation Generation{generationId=1, memberId='consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2', protocol='range'}
2023-11-17 11:05:56.330  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Finished assignment for group at generation 1: {consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2=Assignment(partitions=[samples-topic-0])}
2023-11-17 11:05:56.330  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Finished assignment for group at generation 1: {consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d=Assignment(partitions=[samples-test-topic-0])}
2023-11-17 11:05:56.333  INFO 80454 --- [quest-handler-5] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Assignment received from leader consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2 for group samples-topic for generation 1. The group has 1 members, 0 of which are static.
2023-11-17 11:05:56.333  INFO 80454 --- [quest-handler-6] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Assignment received from leader consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d for group samples-test-id for generation 1. The group has 1 members, 0 of which are static.
2023-11-17 11:05:56.338  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Successfully synced group in generation Generation{generationId=1, memberId='consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2', protocol='range'}
2023-11-17 11:05:56.338  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Successfully synced group in generation Generation{generationId=1, memberId='consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d', protocol='range'}
2023-11-17 11:05:56.338  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Notifying assignor about the new Assignment(partitions=[samples-topic-0])
2023-11-17 11:05:56.338  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Notifying assignor about the new Assignment(partitions=[samples-test-topic-0])
2023-11-17 11:05:56.339  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Adding newly assigned partitions: samples-topic-0
2023-11-17 11:05:56.339  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Adding newly assigned partitions: samples-test-topic-0
2023-11-17 11:05:56.344  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Found no committed offset for partition samples-test-topic-0
2023-11-17 11:05:56.344  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Found no committed offset for partition samples-topic-0
2023-11-17 11:05:56.350  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Resetting offset for partition samples-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:62959 (id: 0 rack: null)], epoch=0}}.
2023-11-17 11:05:56.350  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.SubscriptionState    : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Resetting offset for partition samples-test-topic-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:62959 (id: 0 rack: null)], epoch=0}}.
2023-11-17 11:05:56.350  INFO 80454 --- [les-topic-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : samples-topic: partitions assigned: [samples-topic-0]
2023-11-17 11:05:56.350  INFO 80454 --- [s-test-id-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : samples-test-id: partitions assigned: [samples-test-topic-0]
2023-11-17 11:05:56.370  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Revoke previously assigned partitions samples-topic-0
2023-11-17 11:05:56.370  INFO 80454 --- [les-topic-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : samples-topic: partitions revoked: [samples-topic-0]
2023-11-17 11:05:56.370  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Member consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2 sending LeaveGroup request to coordinator localhost:62959 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-11-17 11:05:56.371  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Resetting generation due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.371  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Request joining group due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.371  INFO 80454 --- [les-topic-0-C-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Unsubscribed all topics or patterns and assigned partitions
2023-11-17 11:05:56.372  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Resetting generation due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.372  INFO 80454 --- [les-topic-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-topic-2, groupId=samples-topic] Request joining group due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.372  INFO 80454 --- [quest-handler-7] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group samples-topic in state PreparingRebalance with old generation 1 (__consumer_offsets-3) (reason: Removing member consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2 on LeaveGroup)
2023-11-17 11:05:56.372  INFO 80454 --- [quest-handler-7] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Group samples-topic with generation 2 is now empty (__consumer_offsets-3)
2023-11-17 11:05:56.373  INFO 80454 --- [quest-handler-7] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-samples-topic-2-f7316434-cbc2-48f4-ab33-792e5b87fdd2, groupInstanceId=None, clientId=consumer-samples-topic-2, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group samples-topic through explicit `LeaveGroup` request
2023-11-17 11:05:56.374  INFO 80454 --- [les-topic-0-C-1] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:05:56.374  INFO 80454 --- [les-topic-0-C-1] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:05:56.374  INFO 80454 --- [les-topic-0-C-1] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:05:56.374  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Revoke previously assigned partitions samples-test-topic-0
2023-11-17 11:05:56.374  INFO 80454 --- [s-test-id-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : samples-test-id: partitions revoked: [samples-test-topic-0]
2023-11-17 11:05:56.374  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Member consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d sending LeaveGroup request to coordinator localhost:62959 (id: 2147483647 rack: null) due to the consumer unsubscribed from all topics
2023-11-17 11:05:56.375  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Resetting generation due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.375  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Request joining group due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.375  INFO 80454 --- [s-test-id-0-C-1] o.a.k.clients.consumer.KafkaConsumer     : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Unsubscribed all topics or patterns and assigned partitions
2023-11-17 11:05:56.375  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Resetting generation due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.375  INFO 80454 --- [s-test-id-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-samples-test-id-1, groupId=samples-test-id] Request joining group due to: consumer pro-actively leaving the group
2023-11-17 11:05:56.375  INFO 80454 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Preparing to rebalance group samples-test-id in state PreparingRebalance with old generation 1 (__consumer_offsets-4) (reason: Removing member consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d on LeaveGroup)
2023-11-17 11:05:56.375  INFO 80454 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Group samples-test-id with generation 2 is now empty (__consumer_offsets-4)
2023-11-17 11:05:56.375  INFO 80454 --- [les-topic-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-samples-topic-2 unregistered
2023-11-17 11:05:56.375  INFO 80454 --- [quest-handler-4] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Member MemberMetadata(memberId=consumer-samples-test-id-1-7ec4cfe9-102f-40f0-b45d-c4baa859e92d, groupInstanceId=None, clientId=consumer-samples-test-id-1, clientHost=/127.0.0.1, sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, supportedProtocols=List(range, cooperative-sticky)) has left group samples-test-id through explicit `LeaveGroup` request
2023-11-17 11:05:56.375  INFO 80454 --- [les-topic-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : samples-topic: Consumer stopped
2023-11-17 11:05:56.376  INFO 80454 --- [s-test-id-0-C-1] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:05:56.376  INFO 80454 --- [s-test-id-0-C-1] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:05:56.376  INFO 80454 --- [s-test-id-0-C-1] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:05:56.377  INFO 80454 --- [s-test-id-0-C-1] o.a.kafka.common.utils.AppInfoParser     : App info kafka.consumer for consumer-samples-test-id-1 unregistered
2023-11-17 11:05:56.377  INFO 80454 --- [s-test-id-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : samples-test-id: Consumer stopped
2023-11-17 11:05:56.377  INFO 80454 --- [    Test worker] io.temporal.worker.WorkerFactory         : shutdown: WorkerFactory{identity=80454@himanshukapoor}
2023-11-17 11:05:56.377  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;UpdateSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.377  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;CustomizeTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.377  INFO 80454 --- [ce=&quot;default&quot;: 4] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.377  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;HelloSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.377  INFO 80454 --- [ce=&quot;default&quot;: 3] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.377  INFO 80454 --- [ce=&quot;default&quot;: 5] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.377  INFO 80454 --- [tdownManager: 1] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Activity Poller taskQueue=&quot;UpdateSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 4] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 5] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 1] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 4] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 2] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 3] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 5] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 2] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 2] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;KafkaSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 3] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 1] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 4] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [tdownManager: 1] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Activity Poller taskQueue=&quot;HelloSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.378  INFO 80454 --- [tdownManager: 1] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Activity Poller taskQueue=&quot;KafkaSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 4] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 3] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 5] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 1] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] shutting down
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 5] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.378  INFO 80454 --- [ce=&quot;default&quot;: 1] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 2] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 5] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [    Test worker] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutting down
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 5] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 1] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 3] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 2] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 2] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 2] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Stopped
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 4] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 3] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 1] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 1] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 3] io.temporal.internal.worker.Poller       : poll loop is terminated: WorkflowPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [ce=&quot;default&quot;: 4] io.temporal.internal.worker.Poller       : poll loop is terminated: ActivityPollTask
2023-11-17 11:05:56.379  INFO 80454 --- [    Test worker] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutdown completed
2023-11-17 11:05:56.379  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2023-11-17 11:05:56.381  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2023-11-17 11:05:56.381  INFO 80454 --- [    Test worker] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shutting down
2023-11-17 11:05:56.382  INFO 80454 --- [    Test worker] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-11-17 11:05:56.382  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutting down
2023-11-17 11:05:56.435  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-11-17 11:05:56.435  INFO 80454 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Stopped
2023-11-17 11:05:56.435  INFO 80454 --- [    Test worker] kafka.server.KafkaApis                   : [KafkaApi-0] Shutdown complete.
2023-11-17 11:05:56.436  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutting down
2023-11-17 11:05:56.635  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutdown completed
2023-11-17 11:05:56.635  INFO 80454 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Stopped
2023-11-17 11:05:56.636  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutting down.
2023-11-17 11:05:56.637  INFO 80454 --- [    Test worker] k.c.transaction.TransactionStateManager  : [Transaction State Manager 0]: Shutdown complete
2023-11-17 11:05:56.637  INFO 80454 --- [    Test worker] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Shutting down
2023-11-17 11:05:56.637  INFO 80454 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Stopped
2023-11-17 11:05:56.637  INFO 80454 --- [    Test worker] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Shutdown completed
2023-11-17 11:05:56.638  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutdown complete.
2023-11-17 11:05:56.638  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutting down.
2023-11-17 11:05:56.639  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutting down
2023-11-17 11:05:56.639  INFO 80454 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Stopped
2023-11-17 11:05:56.639  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-11-17 11:05:56.639  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutting down
2023-11-17 11:05:56.844  INFO 80454 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Stopped
2023-11-17 11:05:56.845  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-11-17 11:05:56.849  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutdown complete.
2023-11-17 11:05:56.851  INFO 80454 --- [    Test worker] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shutting down
2023-11-17 11:05:56.851  INFO 80454 --- [    Test worker] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutting down
2023-11-17 11:05:56.851  INFO 80454 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Stopped
2023-11-17 11:05:56.851  INFO 80454 --- [    Test worker] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutdown completed
2023-11-17 11:05:56.851  INFO 80454 --- [    Test worker] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutting down
2023-11-17 11:05:56.852  INFO 80454 --- [    Test worker] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutdown completed
2023-11-17 11:05:56.853  INFO 80454 --- [    Test worker] kafka.server.ReplicaAlterLogDirsManager  : [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-11-17 11:05:56.853  INFO 80454 --- [    Test worker] kafka.server.ReplicaAlterLogDirsManager  : [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-11-17 11:05:56.853  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutting down
2023-11-17 11:05:57.032  INFO 80454 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Stopped
2023-11-17 11:05:57.032  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutdown completed
2023-11-17 11:05:57.033  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutting down
2023-11-17 11:05:57.236  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutdown completed
2023-11-17 11:05:57.236  INFO 80454 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Stopped
2023-11-17 11:05:57.237  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-11-17 11:05:57.441  INFO 80454 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Stopped
2023-11-17 11:05:57.442  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-11-17 11:05:57.443  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutting down
2023-11-17 11:05:57.639  INFO 80454 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Stopped
2023-11-17 11:05:57.639  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-11-17 11:05:57.644  INFO 80454 --- [    Test worker] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shut down completely
2023-11-17 11:05:57.646  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2023-11-17 11:05:57.647  INFO 80454 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2023-11-17 11:05:57.647  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2023-11-17 11:05:57.649  INFO 80454 --- [    Test worker] k.s.BrokerToControllerChannelManagerImpl : Broker to controller channel manager for alterIsr shutdown
2023-11-17 11:05:57.649  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2023-11-17 11:05:57.649  INFO 80454 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2023-11-17 11:05:57.649  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2023-11-17 11:05:57.649  INFO 80454 --- [    Test worker] k.s.BrokerToControllerChannelManagerImpl : Broker to controller channel manager for forwarding shutdown
2023-11-17 11:05:57.650  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Shutting down.
2023-11-17 11:05:57.650  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : Shutting down the log cleaner.
2023-11-17 11:05:57.650  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Shutting down
2023-11-17 11:05:57.651  INFO 80454 --- [leaner-thread-0] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Stopped
2023-11-17 11:05:57.651  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Shutdown completed
2023-11-17 11:05:57.668  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Shutdown complete.
2023-11-17 11:05:57.668  INFO 80454 --- [    Test worker] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutting down
2023-11-17 11:05:57.668  INFO 80454 --- [    Test worker] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutdown completed
2023-11-17 11:05:57.668  INFO 80454 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Stopped
2023-11-17 11:05:57.669  INFO 80454 --- [    Test worker] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Stopped partition state machine
2023-11-17 11:05:57.669  INFO 80454 --- [    Test worker] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Stopped replica state machine
2023-11-17 11:05:57.670  INFO 80454 --- [    Test worker] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutting down
2023-11-17 11:05:57.670  INFO 80454 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Stopped
2023-11-17 11:05:57.670  INFO 80454 --- [    Test worker] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutdown completed
2023-11-17 11:05:57.671  INFO 80454 --- [    Test worker] kafka.controller.KafkaController         : [Controller id=0] Resigned
2023-11-17 11:05:57.671  INFO 80454 --- [    Test worker] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutting down
2023-11-17 11:05:57.671  INFO 80454 --- [    Test worker] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutdown completed
2023-11-17 11:05:57.671  INFO 80454 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Stopped
2023-11-17 11:05:57.671  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closing.
2023-11-17 11:05:57.779  INFO 80454 --- [ker-EventThread] org.apache.zookeeper.ClientCnxn          : EventThread shut down for session: 0x1003407a75f0000
2023-11-17 11:05:57.779  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Session: 0x1003407a75f0000 closed
2023-11-17 11:05:57.780  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closed.
2023-11-17 11:05:57.780  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutting down
2023-11-17 11:05:57.806  INFO 80454 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Stopped
2023-11-17 11:05:57.806  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-11-17 11:05:57.806  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutting down
2023-11-17 11:05:58.808  INFO 80454 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Stopped
2023-11-17 11:05:58.810  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutdown completed
2023-11-17 11:05:58.810  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutting down
2023-11-17 11:05:58.814  INFO 80454 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Stopped
2023-11-17 11:05:58.814  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutdown completed
2023-11-17 11:05:58.815  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-11-17 11:05:59.818  INFO 80454 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-11-17 11:05:59.818  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-11-17 11:05:59.821  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2023-11-17 11:05:59.838  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2023-11-17 11:05:59.838  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:05:59.838  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:05:59.838  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:05:59.839  INFO 80454 --- [    Test worker] kafka.server.BrokerTopicStats            : Broker and topic stats closed
2023-11-17 11:05:59.839  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : App info kafka.server for 0 unregistered
2023-11-17 11:05:59.840  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] shut down completed
2023-11-17 11:05:59.846  INFO 80454 --- [nnectionExpirer] o.a.z.server.NIOServerCnxnFactory        : ConnnectionExpirerThread interrupted
2023-11-17 11:05:59.847  INFO 80454 --- [electorThread-1] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2023-11-17 11:05:59.847  INFO 80454 --- [electorThread-0] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2023-11-17 11:05:59.847  INFO 80454 --- [ad:/127.0.0.1:0] o.a.z.server.NIOServerCnxnFactory        : accept thread exitted run method
2023-11-17 11:05:59.847  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : shutting down
2023-11-17 11:05:59.847  INFO 80454 --- [    Test worker] o.a.zookeeper.server.RequestThrottler    : Shutting down
2023-11-17 11:05:59.847  INFO 80454 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : Draining request throttler queue
2023-11-17 11:05:59.847  INFO 80454 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : RequestThrottler shutdown. Dropped 0 requests
2023-11-17 11:05:59.847  INFO 80454 --- [    Test worker] o.a.zookeeper.server.SessionTrackerImpl  : Shutting down
2023-11-17 11:05:59.847  INFO 80454 --- [    Test worker] o.a.z.server.PrepRequestProcessor        : Shutting down
2023-11-17 11:05:59.847  INFO 80454 --- [    Test worker] o.a.z.server.SyncRequestProcessor        : Shutting down
2023-11-17 11:05:59.847  INFO 80454 --- [0 cport:62962):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor exited loop!
2023-11-17 11:05:59.847  INFO 80454 --- [   SyncThread:0] o.a.z.server.SyncRequestProcessor        : SyncRequestProcessor exited!
2023-11-17 11:05:59.847  INFO 80454 --- [    Test worker] o.a.z.server.FinalRequestProcessor       : shutdown of request processor complete
2023-11-17 11:05:59.854  INFO 80454 --- [    Test worker] o.a.k.clients.producer.KafkaProducer     : [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
2023-11-17 11:05:59.854  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:05:59.854  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:05:59.855  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:05:59.855  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : App info kafka.producer for producer-1 unregistered
2023-11-17 11:05:59.855  INFO 80454 --- [    Test worker] i.t.s.WorkflowServiceStubsImpl           : shutdown
2023-11-17 11:05:59.855  INFO 80454 --- [    Test worker] i.t.serviceclient.TestServiceStubsImpl   : shutdownNow
2023-11-17 11:05:59.855  INFO 80454 --- [    Test worker] i.t.s.OperatorServiceStubsImpl           : shutdownNow
2023-11-17 11:05:59.855  INFO 80454 --- [    Test worker] io.temporal.worker.WorkerFactory         : shutdownNow: WorkerFactory{identity=80454@himanshukapoor}
2023-11-17 11:05:59.855  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;UpdateSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;CustomizeTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;HelloSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [tdownManager: 1] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Activity Poller taskQueue=&quot;UpdateSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Workflow Poller taskQueue=&quot;KafkaSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] io.temporal.worker.WorkerFactory         : awaitTermination begin: WorkerFactory{identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [tdownManager: 1] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Activity Poller taskQueue=&quot;HelloSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] io.temporal.worker.WorkerFactory         : awaitTermination done: WorkerFactory{identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [tdownManager: 1] io.temporal.internal.worker.Poller       : shutdown: Poller{name=Activity Poller taskQueue=&quot;KafkaSampleTaskQueue&quot;, namespace=&quot;default&quot;, identity=80454@himanshukapoor}
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] i.t.s.WorkflowServiceStubsImpl           : shutdownNow
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] io.temporal.testserver.TestServer        : Shutting down in-process gRPC server
2023-11-17 11:05:59.856  INFO 80454 --- [    Test worker] io.temporal.testserver.TestServer        : Shutting down gRPC Services
2023-11-17 11:05:59.858  INFO 80454 --- [    Test worker] j.LocalContainerEntityManagerFactoryBean : Closing JPA EntityManagerFactory for persistence unit 'default'
2023-11-17 11:05:59.858  INFO 80454 --- [    Test worker] .SchemaDropperImpl$DelayedDropActionImpl : HHH000477: Starting delayed evictData of schema as part of SessionFactory shut-down'
2023-11-17 11:05:59.859  INFO 80454 --- [    Test worker] com.zaxxer.hikari.HikariDataSource       : HikariPool-3 - Shutdown initiated...
2023-11-17 11:05:59.859  INFO 80454 --- [    Test worker] com.zaxxer.hikari.HikariDataSource       : HikariPool-3 - Shutdown completed.
2023-11-17 11:05:59.859  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] shutting down
2023-11-17 11:05:59.860  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Starting controlled shutdown
2023-11-17 11:05:59.863  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Shutting down broker 0
2023-11-17 11:05:59.864  INFO 80454 --- [er-event-thread] state.change.logger                      : [Controller id=0 epoch=1] Sending UpdateMetadata request to brokers HashSet() for 0 partitions
2023-11-17 11:05:59.866  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] Controlled shutdown request returned successfully after 3ms
2023-11-17 11:05:59.866  INFO 80454 --- [    Test worker] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutting down
2023-11-17 11:05:59.866  INFO 80454 --- [-process-thread] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Stopped
2023-11-17 11:05:59.866  INFO 80454 --- [    Test worker] icationListener$ChangeEventProcessThread : [/config/changes-event-process-thread]: Shutdown completed
2023-11-17 11:05:59.866  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopping socket server request processors
2023-11-17 11:05:59.867  INFO 80454 --- [name=forwarding] org.apache.kafka.clients.NetworkClient   : [BrokerToControllerChannelManager broker=0 name=forwarding] Node 0 disconnected.
2023-11-17 11:05:59.867  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Stopped socket server request processors
2023-11-17 11:05:59.867  INFO 80454 --- [    Test worker] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shutting down
2023-11-17 11:05:59.868  INFO 80454 --- [    Test worker] kafka.server.KafkaRequestHandlerPool     : [data-plane Kafka Request Handler on Broker 0], shut down completely
2023-11-17 11:05:59.868  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutting down
2023-11-17 11:05:59.952  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Shutdown completed
2023-11-17 11:05:59.952  INFO 80454 --- [per-0-AlterAcls] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-AlterAcls]: Stopped
2023-11-17 11:05:59.953  INFO 80454 --- [    Test worker] kafka.server.KafkaApis                   : [KafkaApi-0] Shutdown complete.
2023-11-17 11:05:59.953  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutting down
2023-11-17 11:06:00.137  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Shutdown completed
2023-11-17 11:06:00.137  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutting down.
2023-11-17 11:06:00.137  INFO 80454 --- [nReaper-0-topic] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-topic]: Stopped
2023-11-17 11:06:00.139  INFO 80454 --- [    Test worker] k.c.transaction.TransactionStateManager  : [Transaction State Manager 0]: Shutdown complete
2023-11-17 11:06:00.139  INFO 80454 --- [    Test worker] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Shutting down
2023-11-17 11:06:00.139  INFO 80454 --- [rSenderThread-0] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Stopped
2023-11-17 11:06:00.140  INFO 80454 --- [    Test worker] k.c.t.TransactionMarkerChannelManager    : [Transaction Marker Channel Manager 0]: Shutdown completed
2023-11-17 11:06:00.140  INFO 80454 --- [    Test worker] k.c.transaction.TransactionCoordinator   : [TransactionCoordinator id=0] Shutdown complete.
2023-11-17 11:06:00.140  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutting down.
2023-11-17 11:06:00.141  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutting down
2023-11-17 11:06:00.332  INFO 80454 --- [per-0-Heartbeat] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Stopped
2023-11-17 11:06:00.333  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Heartbeat]: Shutdown completed
2023-11-17 11:06:00.334  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutting down
2023-11-17 11:06:00.412  INFO 80454 --- [per-0-Rebalance] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Stopped
2023-11-17 11:06:00.416  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Rebalance]: Shutdown completed
2023-11-17 11:06:00.416  INFO 80454 --- [    Test worker] k.coordinator.group.GroupCoordinator     : [GroupCoordinator 0]: Shutdown complete.
2023-11-17 11:06:00.416  INFO 80454 --- [    Test worker] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shutting down
2023-11-17 11:06:00.417  INFO 80454 --- [    Test worker] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutting down
2023-11-17 11:06:00.417  INFO 80454 --- [rFailureHandler] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Stopped
2023-11-17 11:06:00.417  INFO 80454 --- [    Test worker] k.s.ReplicaManager$LogDirFailureHandler  : [LogDirFailureHandler]: Shutdown completed
2023-11-17 11:06:00.417  INFO 80454 --- [    Test worker] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutting down
2023-11-17 11:06:00.417  INFO 80454 --- [    Test worker] kafka.server.ReplicaFetcherManager       : [ReplicaFetcherManager on broker 0] shutdown completed
2023-11-17 11:06:00.417  INFO 80454 --- [    Test worker] kafka.server.ReplicaAlterLogDirsManager  : [ReplicaAlterLogDirsManager on broker 0] shutting down
2023-11-17 11:06:00.417  INFO 80454 --- [    Test worker] kafka.server.ReplicaAlterLogDirsManager  : [ReplicaAlterLogDirsManager on broker 0] shutdown completed
2023-11-17 11:06:00.417  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutting down
2023-11-17 11:06:00.499  INFO 80454 --- [er-event-thread] kafka.controller.KafkaController         : [Controller id=0] Processing automatic preferred replica leader election
2023-11-17 11:06:00.540  INFO 80454 --- [ SessionTracker] o.a.zookeeper.server.SessionTrackerImpl  : SessionTrackerImpl exited loop!
2023-11-17 11:06:00.548  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Shutdown completed
2023-11-17 11:06:00.548  INFO 80454 --- [nReaper-0-Fetch] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Fetch]: Stopped
2023-11-17 11:06:00.548  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutting down
2023-11-17 11:06:00.695  INFO 80454 --- [eaper-0-Produce] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Stopped
2023-11-17 11:06:00.696  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-Produce]: Shutdown completed
2023-11-17 11:06:00.696  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutting down
2023-11-17 11:06:00.892  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Shutdown completed
2023-11-17 11:06:00.892  INFO 80454 --- [0-DeleteRecords] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-DeleteRecords]: Stopped
2023-11-17 11:06:00.892  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutting down
2023-11-17 11:06:00.919  INFO 80454 --- [    Test worker] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Shutdown completed
2023-11-17 11:06:00.919  INFO 80454 --- [r-0-ElectLeader] perationPurgatory$ExpiredOperationReaper : [ExpirationReaper-0-ElectLeader]: Stopped
2023-11-17 11:06:00.924  INFO 80454 --- [    Test worker] kafka.server.ReplicaManager              : [ReplicaManager broker=0] Shut down completely
2023-11-17 11:06:00.924  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutting down
2023-11-17 11:06:00.924  INFO 80454 --- [0 name=alterIsr] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Stopped
2023-11-17 11:06:00.924  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=alterIsr]: Shutdown completed
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] k.s.BrokerToControllerChannelManagerImpl : Broker to controller channel manager for alterIsr shutdown
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutting down
2023-11-17 11:06:00.925  INFO 80454 --- [name=forwarding] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Stopped
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] k.s.BrokerToControllerRequestThread      : [BrokerToControllerChannelManager broker=0 name=forwarding]: Shutdown completed
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] k.s.BrokerToControllerChannelManagerImpl : Broker to controller channel manager for forwarding shutdown
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Shutting down.
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : Shutting down the log cleaner.
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Shutting down
2023-11-17 11:06:00.925  INFO 80454 --- [leaner-thread-0] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Stopped
2023-11-17 11:06:00.925  INFO 80454 --- [    Test worker] kafka.log.LogCleaner                     : [kafka-log-cleaner-thread-0]: Shutdown completed
2023-11-17 11:06:00.940  INFO 80454 --- [633079906937926] kafka.log.ProducerStateManager           : [ProducerStateManager partition=samples-test-topic-0] Wrote producer snapshot at offset 5 with 1 producer ids in 5 ms.
2023-11-17 11:06:00.954  INFO 80454 --- [633079906937926] kafka.log.ProducerStateManager           : [ProducerStateManager partition=__consumer_offsets-3] Wrote producer snapshot at offset 2 with 0 producer ids in 3 ms.
2023-11-17 11:06:00.980  INFO 80454 --- [633079906937926] kafka.log.ProducerStateManager           : [ProducerStateManager partition=__consumer_offsets-4] Wrote producer snapshot at offset 3 with 0 producer ids in 5 ms.
2023-11-17 11:06:00.997  INFO 80454 --- [    Test worker] kafka.log.LogManager                     : Shutdown complete.
2023-11-17 11:06:00.997  INFO 80454 --- [    Test worker] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutting down
2023-11-17 11:06:00.997  INFO 80454 --- [    Test worker] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Shutdown completed
2023-11-17 11:06:00.997  INFO 80454 --- [er-event-thread] rollerEventManager$ControllerEventThread : [ControllerEventThread controllerId=0] Stopped
2023-11-17 11:06:00.997  INFO 80454 --- [    Test worker] k.controller.ZkPartitionStateMachine     : [PartitionStateMachine controllerId=0] Stopped partition state machine
2023-11-17 11:06:00.998  INFO 80454 --- [    Test worker] kafka.controller.ZkReplicaStateMachine   : [ReplicaStateMachine controllerId=0] Stopped replica state machine
2023-11-17 11:06:00.998  INFO 80454 --- [    Test worker] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutting down
2023-11-17 11:06:00.998  INFO 80454 --- [r-0-send-thread] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Stopped
2023-11-17 11:06:00.998  INFO 80454 --- [    Test worker] kafka.controller.RequestSendThread       : [RequestSendThread controllerId=0] Shutdown completed
2023-11-17 11:06:00.998  INFO 80454 --- [    Test worker] kafka.controller.KafkaController         : [Controller id=0] Resigned
2023-11-17 11:06:00.998  INFO 80454 --- [    Test worker] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutting down
2023-11-17 11:06:00.998  INFO 80454 --- [-process-thread] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Stopped
2023-11-17 11:06:00.998  INFO 80454 --- [    Test worker] stener$ChangeNotificationProcessorThread : [feature-zk-node-event-process-thread]: Shutdown completed
2023-11-17 11:06:00.998  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closing.
2023-11-17 11:06:01.000  WARN 80454 --- [27.0.0.1:62953)] org.apache.zookeeper.ClientCnxn          : An exception was thrown while closing send thread for session 0x1003407a4060000.

org.apache.zookeeper.ClientCnxn$EndOfStreamException: Unable to read additional data from server sessionid 0x1003407a4060000, likely server has closed socket
	at org.apache.zookeeper.ClientCnxnSocketNIO.doIO(ClientCnxnSocketNIO.java:77) ~[zookeeper-3.6.3.jar:3.6.3]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) ~[zookeeper-3.6.3.jar:3.6.3]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290) ~[zookeeper-3.6.3.jar:3.6.3]

2023-11-17 11:06:01.103  INFO 80454 --- [    Test worker] org.apache.zookeeper.ZooKeeper           : Session: 0x1003407a4060000 closed
2023-11-17 11:06:01.103  INFO 80454 --- [ker-EventThread] org.apache.zookeeper.ClientCnxn          : EventThread shut down for session: 0x1003407a4060000
2023-11-17 11:06:01.104  INFO 80454 --- [    Test worker] kafka.zookeeper.ZooKeeperClient          : [ZooKeeperClient Kafka server] Closed.
2023-11-17 11:06:01.104  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutting down
2023-11-17 11:06:01.187  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Shutdown completed
2023-11-17 11:06:01.187  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutting down
2023-11-17 11:06:01.187  INFO 80454 --- [nelReaper-Fetch] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Fetch]: Stopped
2023-11-17 11:06:01.193  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Shutdown completed
2023-11-17 11:06:01.193  INFO 80454 --- [lReaper-Produce] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Produce]: Stopped
2023-11-17 11:06:01.193  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutting down
2023-11-17 11:06:02.198  INFO 80454 --- [lReaper-Request] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Stopped
2023-11-17 11:06:02.199  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-Request]: Shutdown completed
2023-11-17 11:06:02.199  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutting down
2023-11-17 11:06:03.202  INFO 80454 --- [trollerMutation] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Stopped
2023-11-17 11:06:03.202  INFO 80454 --- [    Test worker] lientQuotaManager$ThrottledChannelReaper : [ThrottledChannelReaper-ControllerMutation]: Shutdown completed
2023-11-17 11:06:03.204  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutting down socket server
2023-11-17 11:06:03.215  INFO 80454 --- [    Test worker] kafka.network.SocketServer               : [SocketServer listenerType=ZK_BROKER, nodeId=0] Shutdown completed
2023-11-17 11:06:03.215  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Metrics scheduler closed
2023-11-17 11:06:03.215  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Closing reporter org.apache.kafka.common.metrics.JmxReporter
2023-11-17 11:06:03.215  INFO 80454 --- [    Test worker] org.apache.kafka.common.metrics.Metrics  : Metrics reporters closed
2023-11-17 11:06:03.215  INFO 80454 --- [    Test worker] kafka.server.BrokerTopicStats            : Broker and topic stats closed
2023-11-17 11:06:03.216  INFO 80454 --- [    Test worker] o.a.kafka.common.utils.AppInfoParser     : App info kafka.server for 0 unregistered
2023-11-17 11:06:03.216  INFO 80454 --- [    Test worker] kafka.server.KafkaServer                 : [KafkaServer id=0] shut down completed
2023-11-17 11:06:03.234  INFO 80454 --- [nnectionExpirer] o.a.z.server.NIOServerCnxnFactory        : ConnnectionExpirerThread interrupted
2023-11-17 11:06:03.235  INFO 80454 --- [electorThread-1] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2023-11-17 11:06:03.235  INFO 80454 --- [electorThread-0] o.a.z.server.NIOServerCnxnFactory        : selector thread exitted run method
2023-11-17 11:06:03.235  INFO 80454 --- [ad:/127.0.0.1:0] o.a.z.server.NIOServerCnxnFactory        : accept thread exitted run method
2023-11-17 11:06:03.235  INFO 80454 --- [    Test worker] o.a.zookeeper.server.ZooKeeperServer     : shutting down
2023-11-17 11:06:03.235  INFO 80454 --- [    Test worker] o.a.zookeeper.server.RequestThrottler    : Shutting down
2023-11-17 11:06:03.235  INFO 80454 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : Draining request throttler queue
2023-11-17 11:06:03.235  INFO 80454 --- [equestThrottler] o.a.zookeeper.server.RequestThrottler    : RequestThrottler shutdown. Dropped 0 requests
2023-11-17 11:06:03.235  INFO 80454 --- [    Test worker] o.a.zookeeper.server.SessionTrackerImpl  : Shutting down
2023-11-17 11:06:03.235  INFO 80454 --- [    Test worker] o.a.z.server.PrepRequestProcessor        : Shutting down
2023-11-17 11:06:03.235  INFO 80454 --- [    Test worker] o.a.z.server.SyncRequestProcessor        : Shutting down
2023-11-17 11:06:03.235  INFO 80454 --- [0 cport:62953):] o.a.z.server.PrepRequestProcessor        : PrepRequestProcessor exited loop!
2023-11-17 11:06:03.235  INFO 80454 --- [   SyncThread:0] o.a.z.server.SyncRequestProcessor        : SyncRequestProcessor exited!
2023-11-17 11:06:03.235  INFO 80454 --- [    Test worker] o.a.z.server.FinalRequestProcessor       : shutdown of request processor complete
</pre>
</span>
</div>
</div>
<div id="footer">
<p>
<div>
<label class="hidden" id="label-for-line-wrapping-toggle" for="line-wrapping-toggle">Wrap lines
<input id="line-wrapping-toggle" type="checkbox" autocomplete="off"/>
</label>
</div>Generated by 
<a href="http://www.gradle.org">Gradle 8.0.1</a> at 17 Nov 2023, 11:06:03</p>
</div>
</div>
</body>
</html>
